{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes classifier\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array([np.random.rand(1) + 2 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y2 = np.array([np.random.rand(1) + 2.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y3 = np.array([np.random.rand(1) + 1.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "\n",
    "x1 = np.array(range(len(y1)))\n",
    "x2 = np.array([c+50 for c in range(len(y2))])\n",
    "x3 = np.array([c+80 for c in range(len(y3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data matrix \n",
    "a1 = np.concatenate((x1.reshape(-1,1),y1,np.array([0 for c in range(len(x1))]).reshape(-1,1)), 1).round(4)\n",
    "a2 = np.concatenate((x2.reshape(-1,1),y2,np.array([1 for c in range(len(x2))]).reshape(-1,1)), 1).round(4)\n",
    "a3 = np.concatenate((x3.reshape(-1,1),y3,np.array([2 for c in range(len(x3))]).reshape(-1,1)), 1).round(4)\n",
    "\n",
    "A = np.concatenate((a1,a2,a3), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((241, 3), 71, 84, 86)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, y1.shape[0], y2.shape[0], y3.shape[0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.    ,   2.0265,   0.    ],\n",
       "       [  1.    ,   2.277 ,   0.    ],\n",
       "       [  2.    ,   2.7672,   0.    ],\n",
       "       [  3.    ,   2.7091,   0.    ],\n",
       "       [  4.    ,   2.0278,   0.    ],\n",
       "       [  5.    ,   2.2337,   0.    ],\n",
       "       [  6.    ,   2.2606,   0.    ],\n",
       "       [  7.    ,   2.7312,   0.    ],\n",
       "       [  8.    ,   2.2416,   0.    ],\n",
       "       [  9.    ,   2.747 ,   0.    ],\n",
       "       [ 10.    ,   2.6516,   0.    ],\n",
       "       [ 11.    ,   2.5018,   0.    ],\n",
       "       [ 12.    ,   2.9006,   0.    ],\n",
       "       [ 13.    ,   2.7983,   0.    ],\n",
       "       [ 14.    ,   2.9943,   0.    ],\n",
       "       [ 15.    ,   2.0151,   0.    ],\n",
       "       [ 16.    ,   2.9371,   0.    ],\n",
       "       [ 17.    ,   2.5846,   0.    ],\n",
       "       [ 18.    ,   2.0744,   0.    ],\n",
       "       [ 19.    ,   2.0009,   0.    ],\n",
       "       [ 20.    ,   2.6702,   0.    ],\n",
       "       [ 21.    ,   2.6599,   0.    ],\n",
       "       [ 22.    ,   2.9147,   0.    ],\n",
       "       [ 23.    ,   2.8196,   0.    ],\n",
       "       [ 24.    ,   2.8314,   0.    ],\n",
       "       [ 25.    ,   2.9061,   0.    ],\n",
       "       [ 26.    ,   2.6277,   0.    ],\n",
       "       [ 27.    ,   2.8568,   0.    ],\n",
       "       [ 28.    ,   2.4422,   0.    ],\n",
       "       [ 29.    ,   2.1221,   0.    ],\n",
       "       [ 30.    ,   2.6044,   0.    ],\n",
       "       [ 31.    ,   2.8241,   0.    ],\n",
       "       [ 32.    ,   2.2144,   0.    ],\n",
       "       [ 33.    ,   2.7758,   0.    ],\n",
       "       [ 34.    ,   2.9593,   0.    ],\n",
       "       [ 35.    ,   2.5619,   0.    ],\n",
       "       [ 36.    ,   2.3801,   0.    ],\n",
       "       [ 37.    ,   2.3642,   0.    ],\n",
       "       [ 38.    ,   2.3454,   0.    ],\n",
       "       [ 39.    ,   2.9623,   0.    ],\n",
       "       [ 40.    ,   2.2237,   0.    ],\n",
       "       [ 41.    ,   2.7892,   0.    ],\n",
       "       [ 42.    ,   2.2537,   0.    ],\n",
       "       [ 43.    ,   2.943 ,   0.    ],\n",
       "       [ 44.    ,   2.2847,   0.    ],\n",
       "       [ 45.    ,   2.1871,   0.    ],\n",
       "       [ 46.    ,   2.5437,   0.    ],\n",
       "       [ 47.    ,   2.8691,   0.    ],\n",
       "       [ 48.    ,   2.108 ,   0.    ],\n",
       "       [ 49.    ,   2.7387,   0.    ],\n",
       "       [ 50.    ,   2.3856,   0.    ],\n",
       "       [ 51.    ,   2.0631,   0.    ],\n",
       "       [ 52.    ,   2.264 ,   0.    ],\n",
       "       [ 53.    ,   2.6794,   0.    ],\n",
       "       [ 54.    ,   2.8293,   0.    ],\n",
       "       [ 55.    ,   2.5646,   0.    ],\n",
       "       [ 56.    ,   2.3321,   0.    ],\n",
       "       [ 57.    ,   2.277 ,   0.    ],\n",
       "       [ 58.    ,   2.3763,   0.    ],\n",
       "       [ 59.    ,   2.5315,   0.    ],\n",
       "       [ 60.    ,   2.8005,   0.    ],\n",
       "       [ 61.    ,   2.6155,   0.    ],\n",
       "       [ 62.    ,   2.1017,   0.    ],\n",
       "       [ 63.    ,   2.079 ,   0.    ],\n",
       "       [ 64.    ,   2.6737,   0.    ],\n",
       "       [ 65.    ,   2.432 ,   0.    ],\n",
       "       [ 66.    ,   2.5709,   0.    ],\n",
       "       [ 67.    ,   2.1956,   0.    ],\n",
       "       [ 68.    ,   2.689 ,   0.    ],\n",
       "       [ 69.    ,   2.1036,   0.    ],\n",
       "       [ 70.    ,   2.2608,   0.    ],\n",
       "       [ 50.    ,   3.2144,   1.    ],\n",
       "       [ 51.    ,   3.3767,   1.    ],\n",
       "       [ 52.    ,   2.4639,   1.    ],\n",
       "       [ 53.    ,   3.1608,   1.    ],\n",
       "       [ 54.    ,   2.9834,   1.    ],\n",
       "       [ 55.    ,   2.5951,   1.    ],\n",
       "       [ 56.    ,   2.871 ,   1.    ],\n",
       "       [ 57.    ,   3.0802,   1.    ],\n",
       "       [ 58.    ,   3.2694,   1.    ],\n",
       "       [ 59.    ,   2.6534,   1.    ],\n",
       "       [ 60.    ,   2.9305,   1.    ],\n",
       "       [ 61.    ,   2.8169,   1.    ],\n",
       "       [ 62.    ,   2.4338,   1.    ],\n",
       "       [ 63.    ,   2.6607,   1.    ],\n",
       "       [ 64.    ,   3.0559,   1.    ],\n",
       "       [ 65.    ,   2.4519,   1.    ],\n",
       "       [ 66.    ,   3.169 ,   1.    ],\n",
       "       [ 67.    ,   2.9379,   1.    ],\n",
       "       [ 68.    ,   3.3959,   1.    ],\n",
       "       [ 69.    ,   2.6429,   1.    ],\n",
       "       [ 70.    ,   3.2469,   1.    ],\n",
       "       [ 71.    ,   3.02  ,   1.    ],\n",
       "       [ 72.    ,   3.1755,   1.    ],\n",
       "       [ 73.    ,   3.1113,   1.    ],\n",
       "       [ 74.    ,   2.5434,   1.    ],\n",
       "       [ 75.    ,   3.1626,   1.    ],\n",
       "       [ 76.    ,   3.1813,   1.    ],\n",
       "       [ 77.    ,   3.2721,   1.    ],\n",
       "       [ 78.    ,   3.0472,   1.    ],\n",
       "       [ 79.    ,   3.2389,   1.    ],\n",
       "       [ 80.    ,   2.5316,   1.    ],\n",
       "       [ 81.    ,   2.6648,   1.    ],\n",
       "       [ 82.    ,   2.6969,   1.    ],\n",
       "       [ 83.    ,   2.9245,   1.    ],\n",
       "       [ 84.    ,   2.9461,   1.    ],\n",
       "       [ 85.    ,   3.2768,   1.    ],\n",
       "       [ 86.    ,   2.865 ,   1.    ],\n",
       "       [ 87.    ,   3.3152,   1.    ],\n",
       "       [ 88.    ,   2.5507,   1.    ],\n",
       "       [ 89.    ,   2.7938,   1.    ],\n",
       "       [ 90.    ,   3.3995,   1.    ],\n",
       "       [ 91.    ,   2.9223,   1.    ],\n",
       "       [ 92.    ,   2.9449,   1.    ],\n",
       "       [ 93.    ,   3.1444,   1.    ],\n",
       "       [ 94.    ,   2.7979,   1.    ],\n",
       "       [ 95.    ,   3.2309,   1.    ],\n",
       "       [ 96.    ,   3.0013,   1.    ],\n",
       "       [ 97.    ,   2.8844,   1.    ],\n",
       "       [ 98.    ,   3.0711,   1.    ],\n",
       "       [ 99.    ,   2.6803,   1.    ],\n",
       "       [100.    ,   3.3202,   1.    ],\n",
       "       [101.    ,   2.9473,   1.    ],\n",
       "       [102.    ,   2.8201,   1.    ],\n",
       "       [103.    ,   2.4073,   1.    ],\n",
       "       [104.    ,   3.1762,   1.    ],\n",
       "       [105.    ,   2.9123,   1.    ],\n",
       "       [106.    ,   2.6001,   1.    ],\n",
       "       [107.    ,   3.2401,   1.    ],\n",
       "       [108.    ,   2.7477,   1.    ],\n",
       "       [109.    ,   2.9954,   1.    ],\n",
       "       [110.    ,   3.0251,   1.    ],\n",
       "       [111.    ,   2.9138,   1.    ],\n",
       "       [112.    ,   2.9302,   1.    ],\n",
       "       [113.    ,   3.3753,   1.    ],\n",
       "       [114.    ,   3.2117,   1.    ],\n",
       "       [115.    ,   3.1301,   1.    ],\n",
       "       [116.    ,   2.9422,   1.    ],\n",
       "       [117.    ,   3.1547,   1.    ],\n",
       "       [118.    ,   2.5661,   1.    ],\n",
       "       [119.    ,   2.4423,   1.    ],\n",
       "       [120.    ,   2.964 ,   1.    ],\n",
       "       [121.    ,   3.2602,   1.    ],\n",
       "       [122.    ,   2.5584,   1.    ],\n",
       "       [123.    ,   3.0296,   1.    ],\n",
       "       [124.    ,   2.7959,   1.    ],\n",
       "       [125.    ,   2.7184,   1.    ],\n",
       "       [126.    ,   2.5417,   1.    ],\n",
       "       [127.    ,   2.815 ,   1.    ],\n",
       "       [128.    ,   2.8808,   1.    ],\n",
       "       [129.    ,   2.5247,   1.    ],\n",
       "       [130.    ,   2.6649,   1.    ],\n",
       "       [131.    ,   2.597 ,   1.    ],\n",
       "       [132.    ,   2.7931,   1.    ],\n",
       "       [133.    ,   2.5305,   1.    ],\n",
       "       [ 80.    ,   2.2502,   2.    ],\n",
       "       [ 81.    ,   1.5029,   2.    ],\n",
       "       [ 82.    ,   2.1255,   2.    ],\n",
       "       [ 83.    ,   1.4877,   2.    ],\n",
       "       [ 84.    ,   2.0012,   2.    ],\n",
       "       [ 85.    ,   1.6977,   2.    ],\n",
       "       [ 86.    ,   1.7464,   2.    ],\n",
       "       [ 87.    ,   1.5146,   2.    ],\n",
       "       [ 88.    ,   1.9627,   2.    ],\n",
       "       [ 89.    ,   2.2705,   2.    ],\n",
       "       [ 90.    ,   1.7021,   2.    ],\n",
       "       [ 91.    ,   1.7988,   2.    ],\n",
       "       [ 92.    ,   1.8577,   2.    ],\n",
       "       [ 93.    ,   1.7989,   2.    ],\n",
       "       [ 94.    ,   1.7037,   2.    ],\n",
       "       [ 95.    ,   1.8673,   2.    ],\n",
       "       [ 96.    ,   1.4182,   2.    ],\n",
       "       [ 97.    ,   1.6969,   2.    ],\n",
       "       [ 98.    ,   1.7965,   2.    ],\n",
       "       [ 99.    ,   1.9061,   2.    ],\n",
       "       [100.    ,   1.6632,   2.    ],\n",
       "       [101.    ,   1.8041,   2.    ],\n",
       "       [102.    ,   2.3332,   2.    ],\n",
       "       [103.    ,   1.6374,   2.    ],\n",
       "       [104.    ,   2.2876,   2.    ],\n",
       "       [105.    ,   2.0209,   2.    ],\n",
       "       [106.    ,   1.8884,   2.    ],\n",
       "       [107.    ,   1.6384,   2.    ],\n",
       "       [108.    ,   2.1247,   2.    ],\n",
       "       [109.    ,   2.2341,   2.    ],\n",
       "       [110.    ,   1.6126,   2.    ],\n",
       "       [111.    ,   1.9186,   2.    ],\n",
       "       [112.    ,   1.8487,   2.    ],\n",
       "       [113.    ,   2.0716,   2.    ],\n",
       "       [114.    ,   2.2525,   2.    ],\n",
       "       [115.    ,   1.7618,   2.    ],\n",
       "       [116.    ,   1.9904,   2.    ],\n",
       "       [117.    ,   1.8654,   2.    ],\n",
       "       [118.    ,   2.3103,   2.    ],\n",
       "       [119.    ,   2.2979,   2.    ],\n",
       "       [120.    ,   1.921 ,   2.    ],\n",
       "       [121.    ,   1.8585,   2.    ],\n",
       "       [122.    ,   2.2271,   2.    ],\n",
       "       [123.    ,   1.8574,   2.    ],\n",
       "       [124.    ,   1.9535,   2.    ],\n",
       "       [125.    ,   1.6463,   2.    ],\n",
       "       [126.    ,   1.6891,   2.    ],\n",
       "       [127.    ,   1.9279,   2.    ],\n",
       "       [128.    ,   1.4429,   2.    ],\n",
       "       [129.    ,   1.5954,   2.    ],\n",
       "       [130.    ,   1.6661,   2.    ],\n",
       "       [131.    ,   1.8573,   2.    ],\n",
       "       [132.    ,   1.5123,   2.    ],\n",
       "       [133.    ,   1.5094,   2.    ],\n",
       "       [134.    ,   1.6912,   2.    ],\n",
       "       [135.    ,   2.3619,   2.    ],\n",
       "       [136.    ,   1.4117,   2.    ],\n",
       "       [137.    ,   1.4134,   2.    ],\n",
       "       [138.    ,   1.9898,   2.    ],\n",
       "       [139.    ,   1.4723,   2.    ],\n",
       "       [140.    ,   1.4443,   2.    ],\n",
       "       [141.    ,   2.1663,   2.    ],\n",
       "       [142.    ,   1.4352,   2.    ],\n",
       "       [143.    ,   2.0179,   2.    ],\n",
       "       [144.    ,   1.4227,   2.    ],\n",
       "       [145.    ,   1.7227,   2.    ],\n",
       "       [146.    ,   2.2942,   2.    ],\n",
       "       [147.    ,   1.9286,   2.    ],\n",
       "       [148.    ,   1.7782,   2.    ],\n",
       "       [149.    ,   1.7036,   2.    ],\n",
       "       [150.    ,   2.2045,   2.    ],\n",
       "       [151.    ,   2.0086,   2.    ],\n",
       "       [152.    ,   1.5357,   2.    ],\n",
       "       [153.    ,   1.6537,   2.    ],\n",
       "       [154.    ,   2.3111,   2.    ],\n",
       "       [155.    ,   2.056 ,   2.    ],\n",
       "       [156.    ,   1.6996,   2.    ],\n",
       "       [157.    ,   2.1955,   2.    ],\n",
       "       [158.    ,   2.1581,   2.    ],\n",
       "       [159.    ,   2.0197,   2.    ],\n",
       "       [160.    ,   1.6821,   2.    ],\n",
       "       [161.    ,   1.5385,   2.    ],\n",
       "       [162.    ,   1.6986,   2.    ],\n",
       "       [163.    ,   1.672 ,   2.    ],\n",
       "       [164.    ,   2.0051,   2.    ],\n",
       "       [165.    ,   2.2942,   2.    ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x1, y1, s=10, c='b', marker=\"o\", label='first')\n",
    "ax1.scatter(x2, y2, s=10, c='g', marker=\"o\", label='second')\n",
    "ax1.scatter(x3, y3, s=10, c='r', marker=\"o\", label='third')\n",
    "plt.legend(loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClass:\n",
    "    def __init__(self, A):\n",
    "        self.data = A\n",
    "        self.classes = self.data.label.unique()\n",
    "        self.class_data = self.__separate_data_by_classes__()\n",
    "        self.class_probs = self.__get_probs_of_each_class__()\n",
    "        self.class_means = self.__get_mean_of_each_class__()\n",
    "        self.class_vars = self.__get_variances_class__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __separate_data_by_classes__(self):\n",
    "        c_d = {}\n",
    "        for c in self.classes:\n",
    "            c_d[c] = (self.data.loc[self.data['label'] == c]).values[:,:-1]\n",
    "        return c_d\n",
    "    \n",
    "    def __get_probs_of_each_class__(self):\n",
    "        probs = []\n",
    "        for c in self.classes:\n",
    "            probs.append(len(self.class_data[c])/self.data.shape[0])\n",
    "        return np.array(probs)\n",
    "    \n",
    "    def __get_mean_of_each_class__(self):\n",
    "        means = {}\n",
    "        for c in self.classes:\n",
    "            m = []\n",
    "            for i in range(self.data.shape[1] - 1):\n",
    "                m.append(self.class_data[c][:,i].mean())\n",
    "            means[c] = m\n",
    "        return means\n",
    "    \n",
    "    def __get_variances_class__(self):\n",
    "        variances = {}\n",
    "        for c in self.classes:\n",
    "            va = []\n",
    "            for i in range(self.data.shape[1] - 1):\n",
    "                va.append(self.class_data[c][:,i].var())\n",
    "            variances[c] = va\n",
    "        return variances\n",
    "    \n",
    "    def __get_gaussian_prob__(self, x, mean_, var_):\n",
    "        somnojitels = []\n",
    "        for i in range(x.shape[1]):\n",
    "            x_feat = x[:,i]\n",
    "            somnojitels.append((1/np.sqrt(2*np.pi*var_[i])*np.exp(-1/(2*var_[i])*(x_feat-mean_[i])**2)))\n",
    "        return np.array(somnojitels).prod()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        res = []\n",
    "        for row in x:\n",
    "            pr = []\n",
    "            for c in self.classes:\n",
    "                pr.append(self.class_probs[c]*self.__get_gaussian_prob__(row.reshape(1,-1), self.class_means[c], self.class_vars[c]))\n",
    "            res.append(np.argmax(pr))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(A)\n",
    "df.columns = ['x', 'y', 'label']\n",
    "df = df.astype({'x': 'float32', 'y': 'float32', 'label': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.7672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.7091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x       y  label\n",
       "0  0.0  2.0265      0\n",
       "1  1.0  2.2770      0\n",
       "2  2.0  2.7672      0\n",
       "3  3.0  2.7091      0\n",
       "4  4.0  2.0278      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClass(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29460581, 0.34854772, 0.35684647])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [420.0, 0.08703672342605764],\n",
       " 1: [587.9166666666666, 0.0736548964286419],\n",
       " 2: [616.25, 0.07314999059635043]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.class_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [35.0, 2.5082971881812726],\n",
       " 1: [91.5, 2.920686897777376],\n",
       " 2: [122.5, 1.85254186253215]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.class_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_label = nb.predict(A[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class_label [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print('predicted_class_label', predicted_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.966804979253112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predicted_class_label, A[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(A[:,:-1], A[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2.]\n"
     ]
    }
   ],
   "source": [
    "sklearn_predicted = clf.predict(A[:,:-1])\n",
    "print(sklearn_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.966804979253112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sklearn_predicted, A[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "\n",
    "y1_test = np.array([np.random.rand(1) + 2 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y2_test = np.array([np.random.rand(1) + 2.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y3_test = np.array([np.random.rand(1) + 1.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "\n",
    "x1_test = np.array(range(len(y1_test)))\n",
    "x2_test = np.array([c+50 for c in range(len(y2_test))])\n",
    "x3_test = np.array([c+80 for c in range(len(y3_test))])\n",
    "\n",
    "a1_test = np.concatenate((x1_test.reshape(-1,1),y1_test,np.array([0 for c in range(len(x1_test))]).reshape(-1,1)), 1).round(1)\n",
    "a2_test = np.concatenate((x2_test.reshape(-1,1),y2_test,np.array([1 for c in range(len(x2_test))]).reshape(-1,1)), 1).round(1)\n",
    "a3_test = np.concatenate((x3_test.reshape(-1,1),y3_test,np.array([2 for c in range(len(x3_test))]).reshape(-1,1)), 1).round(1)\n",
    "\n",
    "A_test = np.concatenate((a1_test,a2_test,a3_test), 0)\n",
    "np.random.shuffle(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115. ,   1.6,   2. ],\n",
       "       [134. ,   1.6,   2. ],\n",
       "       [ 64. ,   3.3,   1. ],\n",
       "       [ 27. ,   2.5,   0. ],\n",
       "       [ 58. ,   2.1,   0. ],\n",
       "       [ 17. ,   2.4,   0. ],\n",
       "       [140. ,   3. ,   1. ],\n",
       "       [ 68. ,   3.4,   1. ],\n",
       "       [ 42. ,   2.9,   0. ],\n",
       "       [ 91. ,   2.6,   1. ],\n",
       "       [  5. ,   2.3,   0. ],\n",
       "       [ 81. ,   1.8,   2. ],\n",
       "       [ 77. ,   2.4,   0. ],\n",
       "       [ 43. ,   2.7,   0. ],\n",
       "       [ 97. ,   1.5,   2. ],\n",
       "       [ 54. ,   2.3,   0. ],\n",
       "       [108. ,   1.9,   2. ],\n",
       "       [121. ,   3.2,   1. ],\n",
       "       [ 50. ,   2.7,   0. ],\n",
       "       [ 92. ,   2.3,   2. ],\n",
       "       [100. ,   3.1,   1. ],\n",
       "       [ 34. ,   2.7,   0. ],\n",
       "       [145. ,   2.4,   2. ],\n",
       "       [133. ,   3.2,   1. ],\n",
       "       [139. ,   1.7,   2. ],\n",
       "       [112. ,   1.7,   2. ],\n",
       "       [ 83. ,   2.4,   0. ],\n",
       "       [ 63. ,   3.1,   1. ],\n",
       "       [ 87. ,   1.9,   2. ],\n",
       "       [ 29. ,   2.3,   0. ],\n",
       "       [116. ,   1.7,   2. ],\n",
       "       [ 24. ,   2.3,   0. ],\n",
       "       [ 61. ,   2.4,   0. ],\n",
       "       [128. ,   2.1,   2. ],\n",
       "       [ 19. ,   2.5,   0. ],\n",
       "       [ 92. ,   2.9,   1. ],\n",
       "       [ 96. ,   2.5,   1. ],\n",
       "       [103. ,   2.4,   1. ],\n",
       "       [ 69. ,   3. ,   0. ],\n",
       "       [ 99. ,   2.2,   2. ],\n",
       "       [ 91. ,   1.8,   2. ],\n",
       "       [119. ,   3. ,   1. ],\n",
       "       [120. ,   1.6,   2. ],\n",
       "       [124. ,   3.1,   1. ],\n",
       "       [ 20. ,   2.5,   0. ],\n",
       "       [131. ,   1.6,   2. ],\n",
       "       [ 62. ,   2.7,   0. ],\n",
       "       [ 18. ,   2.1,   0. ],\n",
       "       [125. ,   1.5,   2. ],\n",
       "       [ 45. ,   2.1,   0. ],\n",
       "       [108. ,   2.7,   1. ],\n",
       "       [114. ,   3.1,   1. ],\n",
       "       [ 64. ,   2.3,   0. ],\n",
       "       [  9. ,   2.3,   0. ],\n",
       "       [ 11. ,   2.6,   0. ],\n",
       "       [ 16. ,   2.3,   0. ],\n",
       "       [143. ,   1.6,   2. ],\n",
       "       [  1. ,   2.7,   0. ],\n",
       "       [130. ,   2.5,   1. ],\n",
       "       [ 95. ,   2.5,   1. ],\n",
       "       [138. ,   1.8,   2. ],\n",
       "       [ 89. ,   1.9,   2. ],\n",
       "       [124. ,   2.3,   2. ],\n",
       "       [129. ,   2.5,   1. ],\n",
       "       [ 87. ,   3.3,   1. ],\n",
       "       [116. ,   2.7,   1. ],\n",
       "       [ 70. ,   2. ,   0. ],\n",
       "       [ 80. ,   2.3,   0. ],\n",
       "       [102. ,   3.4,   1. ],\n",
       "       [106. ,   1.8,   2. ],\n",
       "       [ 98. ,   2.6,   1. ],\n",
       "       [ 56. ,   2.9,   0. ],\n",
       "       [  8. ,   2.1,   0. ],\n",
       "       [ 74. ,   2.9,   1. ],\n",
       "       [  2. ,   2.8,   0. ],\n",
       "       [ 62. ,   3. ,   1. ],\n",
       "       [ 23. ,   2.9,   0. ],\n",
       "       [123. ,   3.2,   1. ],\n",
       "       [146. ,   2.3,   2. ],\n",
       "       [127. ,   1.4,   2. ],\n",
       "       [126. ,   2.4,   1. ],\n",
       "       [109. ,   2.1,   2. ],\n",
       "       [ 78. ,   3.2,   1. ],\n",
       "       [ 76. ,   3.3,   1. ],\n",
       "       [134. ,   3.4,   1. ],\n",
       "       [ 61. ,   2.6,   1. ],\n",
       "       [148. ,   2.7,   1. ],\n",
       "       [ 52. ,   2.5,   0. ],\n",
       "       [113. ,   2. ,   2. ],\n",
       "       [102. ,   1.7,   2. ],\n",
       "       [106. ,   3.3,   1. ],\n",
       "       [ 70. ,   3.1,   1. ],\n",
       "       [ 65. ,   2.5,   0. ],\n",
       "       [ 30. ,   2.5,   0. ],\n",
       "       [ 98. ,   2.3,   2. ],\n",
       "       [145. ,   2.9,   1. ],\n",
       "       [ 49. ,   2.7,   0. ],\n",
       "       [ 81. ,   2.6,   0. ],\n",
       "       [  4. ,   2.5,   0. ],\n",
       "       [110. ,   2.7,   1. ],\n",
       "       [ 63. ,   2.4,   0. ],\n",
       "       [137. ,   2.4,   1. ],\n",
       "       [ 67. ,   2.7,   0. ],\n",
       "       [ 68. ,   2.6,   0. ],\n",
       "       [ 41. ,   2.9,   0. ],\n",
       "       [105. ,   3.2,   1. ],\n",
       "       [120. ,   2.4,   1. ],\n",
       "       [ 36. ,   2.6,   0. ],\n",
       "       [113. ,   2.5,   1. ],\n",
       "       [ 82. ,   2.3,   0. ],\n",
       "       [107. ,   1.8,   2. ],\n",
       "       [ 46. ,   2.2,   0. ],\n",
       "       [ 80. ,   1.5,   2. ],\n",
       "       [ 69. ,   2.7,   1. ],\n",
       "       [ 48. ,   2.8,   0. ],\n",
       "       [109. ,   2.7,   1. ],\n",
       "       [139. ,   3.1,   1. ],\n",
       "       [119. ,   2.1,   2. ],\n",
       "       [ 83. ,   2.7,   1. ],\n",
       "       [  7. ,   2.7,   0. ],\n",
       "       [ 35. ,   2.8,   0. ],\n",
       "       [ 81. ,   2.4,   1. ],\n",
       "       [100. ,   1.5,   2. ],\n",
       "       [ 96. ,   1.8,   2. ],\n",
       "       [ 25. ,   2.4,   0. ],\n",
       "       [111. ,   1.7,   2. ],\n",
       "       [132. ,   2. ,   2. ],\n",
       "       [142. ,   1.9,   2. ],\n",
       "       [142. ,   3. ,   1. ],\n",
       "       [ 75. ,   2.8,   1. ],\n",
       "       [ 39. ,   2.4,   0. ],\n",
       "       [101. ,   3.4,   1. ],\n",
       "       [ 60. ,   2.8,   0. ],\n",
       "       [ 50. ,   2.8,   1. ],\n",
       "       [144. ,   2.9,   1. ],\n",
       "       [ 79. ,   2.7,   1. ],\n",
       "       [ 72. ,   2.2,   0. ],\n",
       "       [117. ,   2.9,   1. ],\n",
       "       [ 93. ,   1.5,   2. ],\n",
       "       [ 72. ,   2.9,   1. ],\n",
       "       [ 88. ,   3.3,   1. ],\n",
       "       [ 71. ,   2.9,   1. ],\n",
       "       [101. ,   2.1,   2. ],\n",
       "       [ 77. ,   2.6,   1. ],\n",
       "       [125. ,   3.3,   1. ],\n",
       "       [ 15. ,   2.1,   0. ],\n",
       "       [ 73. ,   3.2,   1. ],\n",
       "       [114. ,   2. ,   2. ],\n",
       "       [ 80. ,   3.2,   1. ],\n",
       "       [ 33. ,   2.3,   0. ],\n",
       "       [122. ,   1.4,   2. ],\n",
       "       [ 37. ,   2.1,   0. ],\n",
       "       [ 59. ,   2.3,   0. ],\n",
       "       [111. ,   2.8,   1. ],\n",
       "       [110. ,   2.1,   2. ],\n",
       "       [ 56. ,   2.6,   1. ],\n",
       "       [143. ,   2.4,   1. ],\n",
       "       [118. ,   2.5,   1. ],\n",
       "       [ 86. ,   1.9,   2. ],\n",
       "       [  0. ,   2.5,   0. ],\n",
       "       [  6. ,   2.4,   0. ],\n",
       "       [ 73. ,   2.5,   0. ],\n",
       "       [135. ,   2.4,   2. ],\n",
       "       [  3. ,   2.2,   0. ],\n",
       "       [ 38. ,   2.7,   0. ],\n",
       "       [132. ,   3.2,   1. ],\n",
       "       [121. ,   2.1,   2. ],\n",
       "       [131. ,   3.4,   1. ],\n",
       "       [ 66. ,   2.5,   1. ],\n",
       "       [117. ,   1.9,   2. ],\n",
       "       [135. ,   3.2,   1. ],\n",
       "       [ 26. ,   2.2,   0. ],\n",
       "       [ 83. ,   1.4,   2. ],\n",
       "       [115. ,   2.8,   1. ],\n",
       "       [112. ,   3.1,   1. ],\n",
       "       [137. ,   2.3,   2. ],\n",
       "       [ 54. ,   3.3,   1. ],\n",
       "       [ 90. ,   2.8,   1. ],\n",
       "       [ 86. ,   3. ,   1. ],\n",
       "       [133. ,   1.5,   2. ],\n",
       "       [ 94. ,   1.6,   2. ],\n",
       "       [ 55. ,   2.4,   0. ],\n",
       "       [123. ,   2. ,   2. ],\n",
       "       [148. ,   1.9,   2. ],\n",
       "       [ 85. ,   3.2,   1. ],\n",
       "       [ 22. ,   2. ,   0. ],\n",
       "       [136. ,   2.9,   1. ],\n",
       "       [ 76. ,   2.2,   0. ],\n",
       "       [104. ,   2.1,   2. ],\n",
       "       [ 57. ,   2.2,   0. ],\n",
       "       [ 84. ,   2.4,   0. ],\n",
       "       [146. ,   2.9,   1. ],\n",
       "       [ 13. ,   2.7,   0. ],\n",
       "       [ 57. ,   2.7,   1. ],\n",
       "       [ 55. ,   2.5,   1. ],\n",
       "       [122. ,   3.2,   1. ],\n",
       "       [129. ,   1.8,   2. ],\n",
       "       [ 84. ,   2.1,   2. ],\n",
       "       [126. ,   2.3,   2. ],\n",
       "       [ 74. ,   2.7,   0. ],\n",
       "       [ 88. ,   1.5,   2. ],\n",
       "       [ 31. ,   2.1,   0. ],\n",
       "       [128. ,   2.6,   1. ],\n",
       "       [ 93. ,   3. ,   1. ],\n",
       "       [ 85. ,   1.8,   2. ],\n",
       "       [ 95. ,   1.5,   2. ],\n",
       "       [ 66. ,   2.1,   0. ],\n",
       "       [ 51. ,   2. ,   0. ],\n",
       "       [ 14. ,   2.2,   0. ],\n",
       "       [144. ,   1.7,   2. ],\n",
       "       [118. ,   1.6,   2. ],\n",
       "       [147. ,   2.8,   1. ],\n",
       "       [ 82. ,   2.5,   1. ],\n",
       "       [ 97. ,   3.3,   1. ],\n",
       "       [ 40. ,   2.2,   0. ],\n",
       "       [ 58. ,   3.3,   1. ],\n",
       "       [ 90. ,   1.7,   2. ],\n",
       "       [ 10. ,   2. ,   0. ],\n",
       "       [136. ,   1.8,   2. ],\n",
       "       [ 32. ,   2.7,   0. ],\n",
       "       [127. ,   2.5,   1. ],\n",
       "       [ 60. ,   3.1,   1. ],\n",
       "       [ 21. ,   2.8,   0. ],\n",
       "       [ 28. ,   2.6,   0. ],\n",
       "       [ 52. ,   3.1,   1. ],\n",
       "       [ 79. ,   2.9,   0. ],\n",
       "       [ 94. ,   3.2,   1. ],\n",
       "       [ 82. ,   1.6,   2. ],\n",
       "       [ 53. ,   3.3,   1. ],\n",
       "       [ 47. ,   2.3,   0. ],\n",
       "       [105. ,   2.4,   2. ],\n",
       "       [130. ,   1.5,   2. ],\n",
       "       [ 71. ,   2.6,   0. ],\n",
       "       [ 99. ,   3. ,   1. ],\n",
       "       [ 12. ,   2.2,   0. ],\n",
       "       [147. ,   1.8,   2. ],\n",
       "       [ 84. ,   3.3,   1. ],\n",
       "       [ 75. ,   2.5,   0. ],\n",
       "       [ 51. ,   2.9,   1. ],\n",
       "       [ 44. ,   2.1,   0. ],\n",
       "       [ 78. ,   2.6,   0. ],\n",
       "       [104. ,   2.9,   1. ],\n",
       "       [ 53. ,   2.9,   0. ],\n",
       "       [140. ,   1.9,   2. ],\n",
       "       [107. ,   2.9,   1. ],\n",
       "       [ 65. ,   2.6,   1. ],\n",
       "       [ 89. ,   2.5,   1. ],\n",
       "       [ 67. ,   2.8,   1. ],\n",
       "       [141. ,   2.3,   2. ],\n",
       "       [103. ,   2.3,   2. ],\n",
       "       [ 59. ,   2.7,   1. ],\n",
       "       [138. ,   2.9,   1. ],\n",
       "       [141. ,   3.2,   1. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = A_test[:,:-1]\n",
    "y_test = A_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8616600790513834"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 1. 0. 0. 0. 1. 1. 0. 1. 0. 2. 1. 0. 2. 0. 2. 1. 0. 2. 1. 0. 2. 1.\n",
      " 2. 2. 1. 1. 2. 0. 2. 0. 0. 2. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1. 0. 2. 1. 0.\n",
      " 2. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 2. 2. 2. 2. 1. 1. 1. 0. 1. 0. 2. 2. 1. 1. 0. 0. 2. 1.\n",
      " 0. 1. 0. 1. 0. 2. 1. 1. 0. 1. 2. 0. 1. 1. 2. 0. 2. 1. 0. 1. 1. 2. 1. 0.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 2. 1. 1. 1. 2. 1.\n",
      " 1. 0. 1. 2. 1. 0. 2. 0. 0. 1. 2. 0. 2. 1. 2. 0. 0. 1. 2. 0. 0. 1. 2. 1.\n",
      " 0. 2. 1. 0. 2. 1. 1. 2. 1. 1. 1. 2. 2. 0. 2. 2. 1. 0. 1. 2. 2. 0. 1. 1.\n",
      " 0. 0. 0. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 2. 0. 0. 0. 2. 2. 1. 1. 1. 0. 1.\n",
      " 2. 0. 2. 0. 1. 1. 0. 0. 1. 1. 1. 2. 1. 0. 1. 2. 1. 1. 0. 2. 1. 1. 0. 0.\n",
      " 1. 1. 1. 2. 1. 0. 1. 1. 2. 2. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "sklearn_predicted_test = clf.predict(X_test)\n",
    "print(sklearn_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8616600790513834"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sklearn_predicted_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64, 19,  2],\n",
       "       [ 9, 86,  4],\n",
       "       [ 0,  1, 68]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(y_pred, y_test):\n",
    "    classes = set(map(int, list(y_pred) + list(y_test)))\n",
    "    conf_m = np.zeros((len(classes), len(classes)))\n",
    "    for class_ in classes:\n",
    "        for class2_ in classes:\n",
    "            conf_m[class_, class2_] = ((np.array(y_pred_test) == class_) * (np.array(y_test) == class2_)).sum()\n",
    "    return conf_m.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64,  9,  0],\n",
       "       [19, 86,  1],\n",
       "       [ 2,  4, 68]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75294118, 0.86868687, 0.98550725])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87671233, 0.81132075, 0.91891892])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.869014547915698"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 2*np.mean(precision) * np.mean(recall)/(np.mean(precision) + np.mean(recall))\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cm = cm[:2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64, 19],\n",
       "       [ 9, 86]], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = binary_cm[0,0]/binary_cm.sum(axis=1)[0]\n",
    "FPR = binary_cm[1,0]/(binary_cm[1,0] + binary_cm[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7710843373493976, 0.09473684210526316)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
