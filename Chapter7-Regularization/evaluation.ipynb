{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob\nimport pydicom\nimport os, os.path as osp\n\nfrom scipy.ndimage.interpolation import zoom\nfrom tqdm import tqdm\n\nimport matplotlib\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport PIL\nimport torch.optim as optim\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv('../input/rsna-str-pulmonary-embolism-detection/sample_submission.csv')\ndf = pd.read_csv('../input/rsna-str-pulmonary-embolism-detection/test.csv')\n\ndicom_folders = list(('../input/rsna-str-pulmonary-embolism-detection/test/' + df.StudyInstanceUID + '/'+ df.SeriesInstanceUID).unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dicom_folders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = transforms.Normalize(\n    mean=[0.5, 0.5, 0.5],\n    std=[0.5, 0.5, 0.5]\n)\n\ntest_ds_trans = transforms.Compose([\n                                transforms.ToPILImage(),\n#                                 transforms.Grayscale(num_output_channels=3),\n                                transforms.Resize((224,224)),\n                                transforms.ToTensor(),\n                                \n#                                 transforms.CenterCrop(224),\n                                normalize\n])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dicom_array(f):\n    dicom_files = glob.glob(osp.join(f, '*.dcm'))\n    dicoms = [pydicom.dcmread(d) for d in dicom_files]\n    M = float(dicoms[0].RescaleSlope)\n    B = float(dicoms[0].RescaleIntercept)\n    # Assume all images are axial\n    z_pos = [float(d.ImagePositionPatient[-1]) for d in dicoms]\n    dicoms = np.asarray([d.pixel_array for d in dicoms])\n    dicoms = dicoms[np.argsort(z_pos)]\n    dicoms = dicoms * M\n    dicoms = dicoms + B\n    return dicoms, np.asarray(dicom_files)[np.argsort(z_pos)]\n\ndef window(img, WL=50, WW=350):\n    upper, lower = WL+WW//2, WL-WW//2\n    X = np.clip(img.copy(), lower, upper)\n    X = X - np.min(X)\n    X = X / np.max(X)\n    X = (X*255.0).astype('uint8')\n    return X\n\nclass Lungs(Dataset):\n    def __init__(self, dicom_folders):\n        self.dicom_folders = dicom_folders\n    def __len__(self): return len(self.dicom_folders)\n    def get(self, i):\n        return load_dicom_array(self.dicom_folders[i])\n    def __getitem__(self, i):\n        try:\n            return self.get(i)\n        except Exception as e:\n            print(e)\n            return None\n\n\n# SAVEDIR = '../../data/train-jpegs/'\nMAX_LENGTH = 256.\n\ndset = Lungs(dicom_folders)\nloader = DataLoader(dset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda x: x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resnet34 = models.resnet34()\n# fc_feat_size = resnet34.fc.in_features\n# resnet34.fc.in_features, resnet34.fc.out_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resnet34.avgpool = Identity()\n# resnet34.fc = Identity()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7.7 - Multi-Task Learning\ndef weights_init(m):\n    classname = m.__class__.__name__\n    for l in m.modules():\n        if isinstance(l, nn.Conv2d):\n            torch.nn.init.uniform_(l.weight)\n        elif isinstance(l, nn.BatchNorm2d):\n            torch.nn.init.uniform_(l.weight)\n        elif isinstance(l, nn.Linear):\n            torch.nn.init.xavier_normal_(l.weight)\n        \nclass MultiTaskHead(nn.Module):\n    def __init__(self):\n        super(MultiTaskHead, self).__init__()\n        self.m = nn.Sequential(\n                nn.Conv2d(512, 512, kernel_size=(1,1), stride=(1,1), bias=False), \n                nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n                nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                nn.Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False),\n                nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                nn.AdaptiveAvgPool2d(output_size=(1,1)),\n        )\n        self.l = nn.Sequential(\n                nn.Linear(2048, fc_feat_size, bias=True), \n            # 7.12 - Dropout\n                nn.Dropout(),\n                nn.ReLU(),\n                nn.Linear(fc_feat_size, 128, bias=True), \n#                 nn.Dropout(),\n#                 nn.Linear(256, 128, bias=True), \n#                 nn.Dropout(),\n                nn.Linear(128, 1), \n#                 nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = x.view(x.shape[0], 512, int(np.sqrt(x.shape[1]/512)), int(np.sqrt(x.shape[1]/512)))\n        x = self.m(x)\n        x = self.l(x.squeeze())\n        return x\n\nclass WrapperModel(nn.Module):\n    def __init__(self, pretrained_model):\n        super(WrapperModel, self).__init__()\n        self.stage = 'train'\n        \n        self.backbone = pretrained_model\n        '''\n        image-level feature\n        '''\n        self.linear_pe_present_on_image = MultiTaskHead()\n        weights_init(self.linear_pe_present_on_image)\n        '''\n        exam-level features\n        '''\n        self.linear_negative_exam_for_pe = MultiTaskHead()\n        weights_init(self.linear_negative_exam_for_pe)\n        \n        self.linear_indeterminate = MultiTaskHead()\n        weights_init(self.linear_indeterminate)\n        \n        self.linear_chronic_pe = MultiTaskHead()\n        weights_init(self.linear_chronic_pe)\n        \n        self.linear_acute_and_chronic_pe = MultiTaskHead()\n        weights_init(self.linear_acute_and_chronic_pe)\n        \n        self.linear_central_pe = MultiTaskHead()\n        weights_init(self.linear_central_pe)\n        \n        self.linear_leftsided_pe = MultiTaskHead()\n        weights_init(self.linear_leftsided_pe)\n        \n        self.linear_rightsided_pe = MultiTaskHead()\n        weights_init(self.linear_rightsided_pe)\n        \n        self.linear_rv_lv_ratio_gte_1 = MultiTaskHead()\n        weights_init(self.linear_rv_lv_ratio_gte_1)\n        \n        self.linear_rv_lv_ratio_lt_1 = MultiTaskHead()\n        weights_init(self.linear_rv_lv_ratio_lt_1)\n        \n#         '''\n#         informational features\n#         '''\n#         self.linear_qa_motion = MultiTaskHead()\n#         weights_init(self.linear_qa_motion)\n        \n#         self.linear_qa_contrast = MultiTaskHead()\n#         weights_init(self.linear_qa_contrast)\n                \n#         self.linear_true_filling_defect_not_pe = MultiTaskHead()\n#         weights_init(self.linear_true_filling_defect_not_pe)\n        \n#         self.linear_flow_artifact = MultiTaskHead()\n#         weights_init(self.linear_flow_artifact)\n\n    def forward(self, x):\n#         if self.stage == 'train':\n#             x, _ = self.backbone(x)\n#         else: \n        \n        x = self.backbone(x)\n        \n#         print('x .shape', x.shape)\n        x_rv_lv_ratio_gte_1 = self.linear_rv_lv_ratio_gte_1(x)\n        x_rv_lv_ratio_lt_1 = self.linear_rv_lv_ratio_lt_1(x)\n        x_leftsided_pe = self.linear_leftsided_pe(x)\n        x_chronic_pe = self.linear_chronic_pe(x)\n        x_rightsided_pe = self.linear_rightsided_pe(x)\n        x_acute_and_chronic_pe = self.linear_acute_and_chronic_pe(x)\n        x_central_pe = self.linear_central_pe(x)\n        x_indeterminate = self.linear_indeterminate(x)\n\n        return torch.cat([x_rv_lv_ratio_gte_1, x_rv_lv_ratio_lt_1, x_leftsided_pe, x_chronic_pe, x_rightsided_pe, x_acute_and_chronic_pe, x_central_pe, x_indeterminate], 1).cpu()\n\n# model = WrapperModel(resnet34).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.load_state_dict(torch.load('../input/resnet342/resnet_1_6473685692657123_0_9219284031043191__0_5446572009448984_0_6214398491481427'))\n# model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.load('../input/model34pth/model.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    for i, data in enumerate(tqdm(loader, total=len(loader))):\n        data = data[0]\n        if type(data) == type(None): continue\n        image, files = data\n        # Windows from https://pubs.rsna.org/doi/pdf/10.1148/rg.245045008\n        image_lung = np.expand_dims(window(image, WL=-600, WW=1500), axis=3)\n        image_mediastinal = np.expand_dims(window(image, WL=40, WW=400), axis=3)\n        image_pe_specific = np.expand_dims(window(image, WL=100, WW=700), axis=3)\n        image = np.concatenate([image_mediastinal, image_pe_specific, image_lung], axis=3)\n        rat = MAX_LENGTH / np.max(image.shape[1:])\n        image = zoom(image, [1.,rat,rat,1.], prefilter=False, order=1)\n\n        images = [test_ds_trans(x_) for x_ in image]\n        x = torch.stack(images).to(device)\n        y_pred = torch.sigmoid(model(x))\n#         print((y_pred>0.5).sum())\n        study_instance = files[0].split('/')[-3]\n        image_names = [f[78:-4] for f in files] # images names in current study_instance\n        indices = [image_names.index(f) for f in sample_df[sample_df['id'].isin(image_names)].id.values.tolist()] # re-index indices\n    \n        '''\n        filling image level predictions\n        '''\n#         values = ((y_pred[indices,:]>0.5).sum(1)>0).type(torch.int8).tolist()\n        values = y_pred.max(1)[0].tolist()\n        sample_df.loc[sample_df['id'].isin(image_names), 'label'] = values\n    \n        '''\n        filling study level predictions\n        '''\n        ress = y_pred.max(0)[0]\n        study_results = ress.tolist()\n#         ress_mean = y_pred.mean(0)\n\n#         ##### consistency ##### \n#         '''\n#         negative\n#         '''\n#         if not any(elem > 0 for elem in values):\n#             negative_ex = 1 - max(study_results)\n#             if ress_mean[-1] > 0.5:\n#                 negative_ex = 0\n#                 study_results = [0]*len(study_results)\n#                 study_results[-1] = 1\n#                 sample_df.loc[sample_df.index[sample_df['id'].str.contains(study_instance)], 'label'] = [negative_ex] + study_results\n#             else:\n#                 negative_ex = 1\n#                 study_results = [0]*len(study_results)\n#                 sample_df.loc[sample_df.index[sample_df['id'].str.contains(study_instance)], 'label'] = [negative_ex] + study_results\n#             continue\n#         '''\n#         only one label for for RV/LV ratio\n#         '''\n#         if study_results[0] + study_results[1] != 1:\n#             if ress_mean[0] > ress_mean[1]:\n#                 study_results[0] = 1\n#                 study_results[1] = 0\n#             else:\n#                 study_results[0] = 0\n#                 study_results[1] = 1\n#         '''\n#         only one label for type\n#         '''\n#         if study_results[3] + study_results[5] != 1:\n#             if ress_mean[3] > ress_mean[5]:\n#                 study_results[3] = 1\n#                 study_results[5] = 0\n#             else:\n#                 study_results[3] = 0\n#                 study_results[5] = 1\n#         '''\n#         at least one label for location\n#         '''\n#         if sum([study_results[i] for i in [2,4,6]]) == 0:\n#             v = [ress_mean[i] for i in [2,4,6]]\n#             max_i = [2,4,6][v.index(max(v))]\n#             study_results[max_i] = 1\n\n#         negative_ex = 0\n        negative_ex = 1 - max(study_results)\n#         print(max(values))\n#         print([negative_ex] + study_results)\n        sample_df.loc[sample_df.index[sample_df['id'].str.contains(study_instance)], 'label'] = [negative_ex] + study_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(sample_df.label < 0).any(), (sample_df.label > 1).any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}