{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement two classification algorithms which are using all the previous material: probabilities, gradient descent, etc.: \n",
    "\n",
    "Logistic regression                                                                                                     \n",
    "https://ru.wikipedia.org/wiki/%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F                                                                                  \n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html                                                     \n",
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array([np.random.rand(1) + 2 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y2 = np.array([np.random.rand(1) + 2.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "\n",
    "x1 = np.array(range(len(y1)))\n",
    "x2 = np.array([c+50 for c in range(len(y2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data matrix \n",
    "a1 = np.concatenate((x1.reshape(-1,1),y1,np.array([1 for c in range(len(x1))]).reshape(-1,1)), 1)\n",
    "a2 = np.concatenate((x2.reshape(-1,1),y2,np.array([0 for c in range(len(x2))]).reshape(-1,1)), 1)\n",
    "\n",
    "A = np.concatenate((a1,a2), 0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194, 3), 98, 96)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, y1.shape[0], y2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,   2.3,   1. ],\n",
       "       [  1. ,   2.4,   1. ],\n",
       "       [  2. ,   2.1,   1. ],\n",
       "       [  3. ,   2.7,   1. ],\n",
       "       [  4. ,   2.7,   1. ],\n",
       "       [  5. ,   2.4,   1. ],\n",
       "       [  6. ,   2.9,   1. ],\n",
       "       [  7. ,   2. ,   1. ],\n",
       "       [  8. ,   2.5,   1. ],\n",
       "       [  9. ,   2.1,   1. ],\n",
       "       [ 10. ,   3. ,   1. ],\n",
       "       [ 11. ,   2.4,   1. ],\n",
       "       [ 12. ,   2.4,   1. ],\n",
       "       [ 13. ,   2. ,   1. ],\n",
       "       [ 14. ,   2.5,   1. ],\n",
       "       [ 15. ,   2. ,   1. ],\n",
       "       [ 16. ,   2.3,   1. ],\n",
       "       [ 17. ,   2.2,   1. ],\n",
       "       [ 18. ,   2.4,   1. ],\n",
       "       [ 19. ,   2. ,   1. ],\n",
       "       [ 20. ,   2.7,   1. ],\n",
       "       [ 21. ,   2.6,   1. ],\n",
       "       [ 22. ,   3. ,   1. ],\n",
       "       [ 23. ,   2.2,   1. ],\n",
       "       [ 24. ,   2.4,   1. ],\n",
       "       [ 25. ,   2. ,   1. ],\n",
       "       [ 26. ,   2.5,   1. ],\n",
       "       [ 27. ,   2.3,   1. ],\n",
       "       [ 28. ,   2.1,   1. ],\n",
       "       [ 29. ,   2.8,   1. ],\n",
       "       [ 30. ,   2.4,   1. ],\n",
       "       [ 31. ,   2.2,   1. ],\n",
       "       [ 32. ,   2. ,   1. ],\n",
       "       [ 33. ,   2.6,   1. ],\n",
       "       [ 34. ,   2.3,   1. ],\n",
       "       [ 35. ,   2.5,   1. ],\n",
       "       [ 36. ,   2.8,   1. ],\n",
       "       [ 37. ,   2.4,   1. ],\n",
       "       [ 38. ,   2.4,   1. ],\n",
       "       [ 39. ,   2.5,   1. ],\n",
       "       [ 40. ,   2. ,   1. ],\n",
       "       [ 41. ,   3. ,   1. ],\n",
       "       [ 42. ,   2.8,   1. ],\n",
       "       [ 43. ,   2.7,   1. ],\n",
       "       [ 44. ,   2.9,   1. ],\n",
       "       [ 45. ,   2.3,   1. ],\n",
       "       [ 46. ,   2. ,   1. ],\n",
       "       [ 47. ,   2.2,   1. ],\n",
       "       [ 48. ,   2.5,   1. ],\n",
       "       [ 49. ,   2.2,   1. ],\n",
       "       [ 50. ,   3. ,   1. ],\n",
       "       [ 51. ,   2.1,   1. ],\n",
       "       [ 52. ,   2.4,   1. ],\n",
       "       [ 53. ,   2.4,   1. ],\n",
       "       [ 54. ,   2.9,   1. ],\n",
       "       [ 55. ,   2.4,   1. ],\n",
       "       [ 56. ,   2.7,   1. ],\n",
       "       [ 57. ,   2.8,   1. ],\n",
       "       [ 58. ,   2.5,   1. ],\n",
       "       [ 59. ,   2.6,   1. ],\n",
       "       [ 60. ,   2.8,   1. ],\n",
       "       [ 61. ,   2.9,   1. ],\n",
       "       [ 62. ,   2.2,   1. ],\n",
       "       [ 63. ,   2.5,   1. ],\n",
       "       [ 64. ,   2.7,   1. ],\n",
       "       [ 65. ,   2.3,   1. ],\n",
       "       [ 66. ,   2.6,   1. ],\n",
       "       [ 67. ,   2.7,   1. ],\n",
       "       [ 68. ,   2.1,   1. ],\n",
       "       [ 69. ,   2.1,   1. ],\n",
       "       [ 70. ,   2.9,   1. ],\n",
       "       [ 71. ,   2.1,   1. ],\n",
       "       [ 72. ,   2.4,   1. ],\n",
       "       [ 73. ,   2.7,   1. ],\n",
       "       [ 74. ,   2.6,   1. ],\n",
       "       [ 75. ,   2.3,   1. ],\n",
       "       [ 76. ,   3. ,   1. ],\n",
       "       [ 77. ,   3. ,   1. ],\n",
       "       [ 78. ,   2.1,   1. ],\n",
       "       [ 79. ,   2.4,   1. ],\n",
       "       [ 80. ,   2.2,   1. ],\n",
       "       [ 81. ,   2.9,   1. ],\n",
       "       [ 82. ,   2.1,   1. ],\n",
       "       [ 83. ,   2.3,   1. ],\n",
       "       [ 84. ,   2.1,   1. ],\n",
       "       [ 85. ,   2. ,   1. ],\n",
       "       [ 86. ,   2.3,   1. ],\n",
       "       [ 87. ,   2.7,   1. ],\n",
       "       [ 88. ,   2.8,   1. ],\n",
       "       [ 89. ,   2.7,   1. ],\n",
       "       [ 90. ,   2.5,   1. ],\n",
       "       [ 91. ,   2.7,   1. ],\n",
       "       [ 92. ,   2.2,   1. ],\n",
       "       [ 93. ,   2. ,   1. ],\n",
       "       [ 94. ,   2.2,   1. ],\n",
       "       [ 95. ,   2.7,   1. ],\n",
       "       [ 96. ,   2.3,   1. ],\n",
       "       [ 97. ,   2.1,   1. ],\n",
       "       [ 50. ,   3.4,   0. ],\n",
       "       [ 51. ,   2.4,   0. ],\n",
       "       [ 52. ,   2.7,   0. ],\n",
       "       [ 53. ,   3.4,   0. ],\n",
       "       [ 54. ,   3.4,   0. ],\n",
       "       [ 55. ,   2.4,   0. ],\n",
       "       [ 56. ,   2.9,   0. ],\n",
       "       [ 57. ,   3. ,   0. ],\n",
       "       [ 58. ,   2.7,   0. ],\n",
       "       [ 59. ,   2.6,   0. ],\n",
       "       [ 60. ,   2.8,   0. ],\n",
       "       [ 61. ,   3.1,   0. ],\n",
       "       [ 62. ,   2.4,   0. ],\n",
       "       [ 63. ,   3. ,   0. ],\n",
       "       [ 64. ,   2.5,   0. ],\n",
       "       [ 65. ,   2.7,   0. ],\n",
       "       [ 66. ,   2.6,   0. ],\n",
       "       [ 67. ,   3. ,   0. ],\n",
       "       [ 68. ,   2.7,   0. ],\n",
       "       [ 69. ,   2.8,   0. ],\n",
       "       [ 70. ,   2.5,   0. ],\n",
       "       [ 71. ,   2.5,   0. ],\n",
       "       [ 72. ,   2.4,   0. ],\n",
       "       [ 73. ,   2.5,   0. ],\n",
       "       [ 74. ,   2.8,   0. ],\n",
       "       [ 75. ,   3. ,   0. ],\n",
       "       [ 76. ,   3.2,   0. ],\n",
       "       [ 77. ,   2.6,   0. ],\n",
       "       [ 78. ,   3.4,   0. ],\n",
       "       [ 79. ,   2.4,   0. ],\n",
       "       [ 80. ,   2.7,   0. ],\n",
       "       [ 81. ,   2.5,   0. ],\n",
       "       [ 82. ,   3. ,   0. ],\n",
       "       [ 83. ,   3. ,   0. ],\n",
       "       [ 84. ,   2.9,   0. ],\n",
       "       [ 85. ,   3.1,   0. ],\n",
       "       [ 86. ,   2.6,   0. ],\n",
       "       [ 87. ,   3.1,   0. ],\n",
       "       [ 88. ,   2.5,   0. ],\n",
       "       [ 89. ,   2.6,   0. ],\n",
       "       [ 90. ,   2.9,   0. ],\n",
       "       [ 91. ,   2.5,   0. ],\n",
       "       [ 92. ,   2.8,   0. ],\n",
       "       [ 93. ,   2.9,   0. ],\n",
       "       [ 94. ,   3.1,   0. ],\n",
       "       [ 95. ,   3.1,   0. ],\n",
       "       [ 96. ,   2.5,   0. ],\n",
       "       [ 97. ,   3.3,   0. ],\n",
       "       [ 98. ,   2.7,   0. ],\n",
       "       [ 99. ,   2.8,   0. ],\n",
       "       [100. ,   2.9,   0. ],\n",
       "       [101. ,   3. ,   0. ],\n",
       "       [102. ,   2.5,   0. ],\n",
       "       [103. ,   2.8,   0. ],\n",
       "       [104. ,   3.3,   0. ],\n",
       "       [105. ,   3.2,   0. ],\n",
       "       [106. ,   3.3,   0. ],\n",
       "       [107. ,   3.3,   0. ],\n",
       "       [108. ,   2.5,   0. ],\n",
       "       [109. ,   2.9,   0. ],\n",
       "       [110. ,   2.9,   0. ],\n",
       "       [111. ,   3.3,   0. ],\n",
       "       [112. ,   3.1,   0. ],\n",
       "       [113. ,   2.7,   0. ],\n",
       "       [114. ,   2.8,   0. ],\n",
       "       [115. ,   2.7,   0. ],\n",
       "       [116. ,   2.5,   0. ],\n",
       "       [117. ,   3.2,   0. ],\n",
       "       [118. ,   2.9,   0. ],\n",
       "       [119. ,   2.5,   0. ],\n",
       "       [120. ,   3.1,   0. ],\n",
       "       [121. ,   2.8,   0. ],\n",
       "       [122. ,   3.3,   0. ],\n",
       "       [123. ,   2.9,   0. ],\n",
       "       [124. ,   3.3,   0. ],\n",
       "       [125. ,   3. ,   0. ],\n",
       "       [126. ,   2.5,   0. ],\n",
       "       [127. ,   2.6,   0. ],\n",
       "       [128. ,   3.1,   0. ],\n",
       "       [129. ,   3.3,   0. ],\n",
       "       [130. ,   2.7,   0. ],\n",
       "       [131. ,   3.3,   0. ],\n",
       "       [132. ,   3. ,   0. ],\n",
       "       [133. ,   3. ,   0. ],\n",
       "       [134. ,   2.8,   0. ],\n",
       "       [135. ,   3.2,   0. ],\n",
       "       [136. ,   3.1,   0. ],\n",
       "       [137. ,   3.3,   0. ],\n",
       "       [138. ,   2.8,   0. ],\n",
       "       [139. ,   2.9,   0. ],\n",
       "       [140. ,   2.7,   0. ],\n",
       "       [141. ,   2.5,   0. ],\n",
       "       [142. ,   2.7,   0. ],\n",
       "       [143. ,   2.5,   0. ],\n",
       "       [144. ,   3.4,   0. ],\n",
       "       [145. ,   2.8,   0. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x1, y1, s=10, c='b', marker=\"o\", label='first')\n",
    "ax1.scatter(x2, y2, s=10, c='g', marker=\"o\", label='second')\n",
    "plt.legend(loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,   2.3,   1. ,   1. ],\n",
       "       [  1. ,   2.4,   1. ,   1. ],\n",
       "       [  2. ,   2.1,   1. ,   1. ],\n",
       "       [  3. ,   2.7,   1. ,   1. ],\n",
       "       [  4. ,   2.7,   1. ,   1. ],\n",
       "       [  5. ,   2.4,   1. ,   1. ],\n",
       "       [  6. ,   2.9,   1. ,   1. ],\n",
       "       [  7. ,   2. ,   1. ,   1. ],\n",
       "       [  8. ,   2.5,   1. ,   1. ],\n",
       "       [  9. ,   2.1,   1. ,   1. ],\n",
       "       [ 10. ,   3. ,   1. ,   1. ],\n",
       "       [ 11. ,   2.4,   1. ,   1. ],\n",
       "       [ 12. ,   2.4,   1. ,   1. ],\n",
       "       [ 13. ,   2. ,   1. ,   1. ],\n",
       "       [ 14. ,   2.5,   1. ,   1. ],\n",
       "       [ 15. ,   2. ,   1. ,   1. ],\n",
       "       [ 16. ,   2.3,   1. ,   1. ],\n",
       "       [ 17. ,   2.2,   1. ,   1. ],\n",
       "       [ 18. ,   2.4,   1. ,   1. ],\n",
       "       [ 19. ,   2. ,   1. ,   1. ],\n",
       "       [ 20. ,   2.7,   1. ,   1. ],\n",
       "       [ 21. ,   2.6,   1. ,   1. ],\n",
       "       [ 22. ,   3. ,   1. ,   1. ],\n",
       "       [ 23. ,   2.2,   1. ,   1. ],\n",
       "       [ 24. ,   2.4,   1. ,   1. ],\n",
       "       [ 25. ,   2. ,   1. ,   1. ],\n",
       "       [ 26. ,   2.5,   1. ,   1. ],\n",
       "       [ 27. ,   2.3,   1. ,   1. ],\n",
       "       [ 28. ,   2.1,   1. ,   1. ],\n",
       "       [ 29. ,   2.8,   1. ,   1. ],\n",
       "       [ 30. ,   2.4,   1. ,   1. ],\n",
       "       [ 31. ,   2.2,   1. ,   1. ],\n",
       "       [ 32. ,   2. ,   1. ,   1. ],\n",
       "       [ 33. ,   2.6,   1. ,   1. ],\n",
       "       [ 34. ,   2.3,   1. ,   1. ],\n",
       "       [ 35. ,   2.5,   1. ,   1. ],\n",
       "       [ 36. ,   2.8,   1. ,   1. ],\n",
       "       [ 37. ,   2.4,   1. ,   1. ],\n",
       "       [ 38. ,   2.4,   1. ,   1. ],\n",
       "       [ 39. ,   2.5,   1. ,   1. ],\n",
       "       [ 40. ,   2. ,   1. ,   1. ],\n",
       "       [ 41. ,   3. ,   1. ,   1. ],\n",
       "       [ 42. ,   2.8,   1. ,   1. ],\n",
       "       [ 43. ,   2.7,   1. ,   1. ],\n",
       "       [ 44. ,   2.9,   1. ,   1. ],\n",
       "       [ 45. ,   2.3,   1. ,   1. ],\n",
       "       [ 46. ,   2. ,   1. ,   1. ],\n",
       "       [ 47. ,   2.2,   1. ,   1. ],\n",
       "       [ 48. ,   2.5,   1. ,   1. ],\n",
       "       [ 49. ,   2.2,   1. ,   1. ],\n",
       "       [ 50. ,   3. ,   1. ,   1. ],\n",
       "       [ 51. ,   2.1,   1. ,   1. ],\n",
       "       [ 52. ,   2.4,   1. ,   1. ],\n",
       "       [ 53. ,   2.4,   1. ,   1. ],\n",
       "       [ 54. ,   2.9,   1. ,   1. ],\n",
       "       [ 55. ,   2.4,   1. ,   1. ],\n",
       "       [ 56. ,   2.7,   1. ,   1. ],\n",
       "       [ 57. ,   2.8,   1. ,   1. ],\n",
       "       [ 58. ,   2.5,   1. ,   1. ],\n",
       "       [ 59. ,   2.6,   1. ,   1. ],\n",
       "       [ 60. ,   2.8,   1. ,   1. ],\n",
       "       [ 61. ,   2.9,   1. ,   1. ],\n",
       "       [ 62. ,   2.2,   1. ,   1. ],\n",
       "       [ 63. ,   2.5,   1. ,   1. ],\n",
       "       [ 64. ,   2.7,   1. ,   1. ],\n",
       "       [ 65. ,   2.3,   1. ,   1. ],\n",
       "       [ 66. ,   2.6,   1. ,   1. ],\n",
       "       [ 67. ,   2.7,   1. ,   1. ],\n",
       "       [ 68. ,   2.1,   1. ,   1. ],\n",
       "       [ 69. ,   2.1,   1. ,   1. ],\n",
       "       [ 70. ,   2.9,   1. ,   1. ],\n",
       "       [ 71. ,   2.1,   1. ,   1. ],\n",
       "       [ 72. ,   2.4,   1. ,   1. ],\n",
       "       [ 73. ,   2.7,   1. ,   1. ],\n",
       "       [ 74. ,   2.6,   1. ,   1. ],\n",
       "       [ 75. ,   2.3,   1. ,   1. ],\n",
       "       [ 76. ,   3. ,   1. ,   1. ],\n",
       "       [ 77. ,   3. ,   1. ,   1. ],\n",
       "       [ 78. ,   2.1,   1. ,   1. ],\n",
       "       [ 79. ,   2.4,   1. ,   1. ],\n",
       "       [ 80. ,   2.2,   1. ,   1. ],\n",
       "       [ 81. ,   2.9,   1. ,   1. ],\n",
       "       [ 82. ,   2.1,   1. ,   1. ],\n",
       "       [ 83. ,   2.3,   1. ,   1. ],\n",
       "       [ 84. ,   2.1,   1. ,   1. ],\n",
       "       [ 85. ,   2. ,   1. ,   1. ],\n",
       "       [ 86. ,   2.3,   1. ,   1. ],\n",
       "       [ 87. ,   2.7,   1. ,   1. ],\n",
       "       [ 88. ,   2.8,   1. ,   1. ],\n",
       "       [ 89. ,   2.7,   1. ,   1. ],\n",
       "       [ 90. ,   2.5,   1. ,   1. ],\n",
       "       [ 91. ,   2.7,   1. ,   1. ],\n",
       "       [ 92. ,   2.2,   1. ,   1. ],\n",
       "       [ 93. ,   2. ,   1. ,   1. ],\n",
       "       [ 94. ,   2.2,   1. ,   1. ],\n",
       "       [ 95. ,   2.7,   1. ,   1. ],\n",
       "       [ 96. ,   2.3,   1. ,   1. ],\n",
       "       [ 97. ,   2.1,   1. ,   1. ],\n",
       "       [ 50. ,   3.4,   0. ,   1. ],\n",
       "       [ 51. ,   2.4,   0. ,   1. ],\n",
       "       [ 52. ,   2.7,   0. ,   1. ],\n",
       "       [ 53. ,   3.4,   0. ,   1. ],\n",
       "       [ 54. ,   3.4,   0. ,   1. ],\n",
       "       [ 55. ,   2.4,   0. ,   1. ],\n",
       "       [ 56. ,   2.9,   0. ,   1. ],\n",
       "       [ 57. ,   3. ,   0. ,   1. ],\n",
       "       [ 58. ,   2.7,   0. ,   1. ],\n",
       "       [ 59. ,   2.6,   0. ,   1. ],\n",
       "       [ 60. ,   2.8,   0. ,   1. ],\n",
       "       [ 61. ,   3.1,   0. ,   1. ],\n",
       "       [ 62. ,   2.4,   0. ,   1. ],\n",
       "       [ 63. ,   3. ,   0. ,   1. ],\n",
       "       [ 64. ,   2.5,   0. ,   1. ],\n",
       "       [ 65. ,   2.7,   0. ,   1. ],\n",
       "       [ 66. ,   2.6,   0. ,   1. ],\n",
       "       [ 67. ,   3. ,   0. ,   1. ],\n",
       "       [ 68. ,   2.7,   0. ,   1. ],\n",
       "       [ 69. ,   2.8,   0. ,   1. ],\n",
       "       [ 70. ,   2.5,   0. ,   1. ],\n",
       "       [ 71. ,   2.5,   0. ,   1. ],\n",
       "       [ 72. ,   2.4,   0. ,   1. ],\n",
       "       [ 73. ,   2.5,   0. ,   1. ],\n",
       "       [ 74. ,   2.8,   0. ,   1. ],\n",
       "       [ 75. ,   3. ,   0. ,   1. ],\n",
       "       [ 76. ,   3.2,   0. ,   1. ],\n",
       "       [ 77. ,   2.6,   0. ,   1. ],\n",
       "       [ 78. ,   3.4,   0. ,   1. ],\n",
       "       [ 79. ,   2.4,   0. ,   1. ],\n",
       "       [ 80. ,   2.7,   0. ,   1. ],\n",
       "       [ 81. ,   2.5,   0. ,   1. ],\n",
       "       [ 82. ,   3. ,   0. ,   1. ],\n",
       "       [ 83. ,   3. ,   0. ,   1. ],\n",
       "       [ 84. ,   2.9,   0. ,   1. ],\n",
       "       [ 85. ,   3.1,   0. ,   1. ],\n",
       "       [ 86. ,   2.6,   0. ,   1. ],\n",
       "       [ 87. ,   3.1,   0. ,   1. ],\n",
       "       [ 88. ,   2.5,   0. ,   1. ],\n",
       "       [ 89. ,   2.6,   0. ,   1. ],\n",
       "       [ 90. ,   2.9,   0. ,   1. ],\n",
       "       [ 91. ,   2.5,   0. ,   1. ],\n",
       "       [ 92. ,   2.8,   0. ,   1. ],\n",
       "       [ 93. ,   2.9,   0. ,   1. ],\n",
       "       [ 94. ,   3.1,   0. ,   1. ],\n",
       "       [ 95. ,   3.1,   0. ,   1. ],\n",
       "       [ 96. ,   2.5,   0. ,   1. ],\n",
       "       [ 97. ,   3.3,   0. ,   1. ],\n",
       "       [ 98. ,   2.7,   0. ,   1. ],\n",
       "       [ 99. ,   2.8,   0. ,   1. ],\n",
       "       [100. ,   2.9,   0. ,   1. ],\n",
       "       [101. ,   3. ,   0. ,   1. ],\n",
       "       [102. ,   2.5,   0. ,   1. ],\n",
       "       [103. ,   2.8,   0. ,   1. ],\n",
       "       [104. ,   3.3,   0. ,   1. ],\n",
       "       [105. ,   3.2,   0. ,   1. ],\n",
       "       [106. ,   3.3,   0. ,   1. ],\n",
       "       [107. ,   3.3,   0. ,   1. ],\n",
       "       [108. ,   2.5,   0. ,   1. ],\n",
       "       [109. ,   2.9,   0. ,   1. ],\n",
       "       [110. ,   2.9,   0. ,   1. ],\n",
       "       [111. ,   3.3,   0. ,   1. ],\n",
       "       [112. ,   3.1,   0. ,   1. ],\n",
       "       [113. ,   2.7,   0. ,   1. ],\n",
       "       [114. ,   2.8,   0. ,   1. ],\n",
       "       [115. ,   2.7,   0. ,   1. ],\n",
       "       [116. ,   2.5,   0. ,   1. ],\n",
       "       [117. ,   3.2,   0. ,   1. ],\n",
       "       [118. ,   2.9,   0. ,   1. ],\n",
       "       [119. ,   2.5,   0. ,   1. ],\n",
       "       [120. ,   3.1,   0. ,   1. ],\n",
       "       [121. ,   2.8,   0. ,   1. ],\n",
       "       [122. ,   3.3,   0. ,   1. ],\n",
       "       [123. ,   2.9,   0. ,   1. ],\n",
       "       [124. ,   3.3,   0. ,   1. ],\n",
       "       [125. ,   3. ,   0. ,   1. ],\n",
       "       [126. ,   2.5,   0. ,   1. ],\n",
       "       [127. ,   2.6,   0. ,   1. ],\n",
       "       [128. ,   3.1,   0. ,   1. ],\n",
       "       [129. ,   3.3,   0. ,   1. ],\n",
       "       [130. ,   2.7,   0. ,   1. ],\n",
       "       [131. ,   3.3,   0. ,   1. ],\n",
       "       [132. ,   3. ,   0. ,   1. ],\n",
       "       [133. ,   3. ,   0. ,   1. ],\n",
       "       [134. ,   2.8,   0. ,   1. ],\n",
       "       [135. ,   3.2,   0. ,   1. ],\n",
       "       [136. ,   3.1,   0. ,   1. ],\n",
       "       [137. ,   3.3,   0. ,   1. ],\n",
       "       [138. ,   2.8,   0. ,   1. ],\n",
       "       [139. ,   2.9,   0. ,   1. ],\n",
       "       [140. ,   2.7,   0. ,   1. ],\n",
       "       [141. ,   2.5,   0. ,   1. ],\n",
       "       [142. ,   2.7,   0. ,   1. ],\n",
       "       [143. ,   2.5,   0. ,   1. ],\n",
       "       [144. ,   3.4,   0. ,   1. ],\n",
       "       [145. ,   2.8,   0. ,   1. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((A, np.ones((A.shape[0], 1), dtype=A.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.ones((A.shape[0], A.shape[1]+1))\n",
    "temp[:,1:] = A\n",
    "X = temp[:,:-1]\n",
    "y = temp[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. ,   0. ,   2.3],\n",
       "       [  1. ,   1. ,   2.4],\n",
       "       [  1. ,   2. ,   2.1],\n",
       "       [  1. ,   3. ,   2.7],\n",
       "       [  1. ,   4. ,   2.7],\n",
       "       [  1. ,   5. ,   2.4],\n",
       "       [  1. ,   6. ,   2.9],\n",
       "       [  1. ,   7. ,   2. ],\n",
       "       [  1. ,   8. ,   2.5],\n",
       "       [  1. ,   9. ,   2.1],\n",
       "       [  1. ,  10. ,   3. ],\n",
       "       [  1. ,  11. ,   2.4],\n",
       "       [  1. ,  12. ,   2.4],\n",
       "       [  1. ,  13. ,   2. ],\n",
       "       [  1. ,  14. ,   2.5],\n",
       "       [  1. ,  15. ,   2. ],\n",
       "       [  1. ,  16. ,   2.3],\n",
       "       [  1. ,  17. ,   2.2],\n",
       "       [  1. ,  18. ,   2.4],\n",
       "       [  1. ,  19. ,   2. ],\n",
       "       [  1. ,  20. ,   2.7],\n",
       "       [  1. ,  21. ,   2.6],\n",
       "       [  1. ,  22. ,   3. ],\n",
       "       [  1. ,  23. ,   2.2],\n",
       "       [  1. ,  24. ,   2.4],\n",
       "       [  1. ,  25. ,   2. ],\n",
       "       [  1. ,  26. ,   2.5],\n",
       "       [  1. ,  27. ,   2.3],\n",
       "       [  1. ,  28. ,   2.1],\n",
       "       [  1. ,  29. ,   2.8],\n",
       "       [  1. ,  30. ,   2.4],\n",
       "       [  1. ,  31. ,   2.2],\n",
       "       [  1. ,  32. ,   2. ],\n",
       "       [  1. ,  33. ,   2.6],\n",
       "       [  1. ,  34. ,   2.3],\n",
       "       [  1. ,  35. ,   2.5],\n",
       "       [  1. ,  36. ,   2.8],\n",
       "       [  1. ,  37. ,   2.4],\n",
       "       [  1. ,  38. ,   2.4],\n",
       "       [  1. ,  39. ,   2.5],\n",
       "       [  1. ,  40. ,   2. ],\n",
       "       [  1. ,  41. ,   3. ],\n",
       "       [  1. ,  42. ,   2.8],\n",
       "       [  1. ,  43. ,   2.7],\n",
       "       [  1. ,  44. ,   2.9],\n",
       "       [  1. ,  45. ,   2.3],\n",
       "       [  1. ,  46. ,   2. ],\n",
       "       [  1. ,  47. ,   2.2],\n",
       "       [  1. ,  48. ,   2.5],\n",
       "       [  1. ,  49. ,   2.2],\n",
       "       [  1. ,  50. ,   3. ],\n",
       "       [  1. ,  51. ,   2.1],\n",
       "       [  1. ,  52. ,   2.4],\n",
       "       [  1. ,  53. ,   2.4],\n",
       "       [  1. ,  54. ,   2.9],\n",
       "       [  1. ,  55. ,   2.4],\n",
       "       [  1. ,  56. ,   2.7],\n",
       "       [  1. ,  57. ,   2.8],\n",
       "       [  1. ,  58. ,   2.5],\n",
       "       [  1. ,  59. ,   2.6],\n",
       "       [  1. ,  60. ,   2.8],\n",
       "       [  1. ,  61. ,   2.9],\n",
       "       [  1. ,  62. ,   2.2],\n",
       "       [  1. ,  63. ,   2.5],\n",
       "       [  1. ,  64. ,   2.7],\n",
       "       [  1. ,  65. ,   2.3],\n",
       "       [  1. ,  66. ,   2.6],\n",
       "       [  1. ,  67. ,   2.7],\n",
       "       [  1. ,  68. ,   2.1],\n",
       "       [  1. ,  69. ,   2.1],\n",
       "       [  1. ,  70. ,   2.9],\n",
       "       [  1. ,  71. ,   2.1],\n",
       "       [  1. ,  72. ,   2.4],\n",
       "       [  1. ,  73. ,   2.7],\n",
       "       [  1. ,  74. ,   2.6],\n",
       "       [  1. ,  75. ,   2.3],\n",
       "       [  1. ,  76. ,   3. ],\n",
       "       [  1. ,  77. ,   3. ],\n",
       "       [  1. ,  78. ,   2.1],\n",
       "       [  1. ,  79. ,   2.4],\n",
       "       [  1. ,  80. ,   2.2],\n",
       "       [  1. ,  81. ,   2.9],\n",
       "       [  1. ,  82. ,   2.1],\n",
       "       [  1. ,  83. ,   2.3],\n",
       "       [  1. ,  84. ,   2.1],\n",
       "       [  1. ,  85. ,   2. ],\n",
       "       [  1. ,  86. ,   2.3],\n",
       "       [  1. ,  87. ,   2.7],\n",
       "       [  1. ,  88. ,   2.8],\n",
       "       [  1. ,  89. ,   2.7],\n",
       "       [  1. ,  90. ,   2.5],\n",
       "       [  1. ,  91. ,   2.7],\n",
       "       [  1. ,  92. ,   2.2],\n",
       "       [  1. ,  93. ,   2. ],\n",
       "       [  1. ,  94. ,   2.2],\n",
       "       [  1. ,  95. ,   2.7],\n",
       "       [  1. ,  96. ,   2.3],\n",
       "       [  1. ,  97. ,   2.1],\n",
       "       [  1. ,  50. ,   3.4],\n",
       "       [  1. ,  51. ,   2.4],\n",
       "       [  1. ,  52. ,   2.7],\n",
       "       [  1. ,  53. ,   3.4],\n",
       "       [  1. ,  54. ,   3.4],\n",
       "       [  1. ,  55. ,   2.4],\n",
       "       [  1. ,  56. ,   2.9],\n",
       "       [  1. ,  57. ,   3. ],\n",
       "       [  1. ,  58. ,   2.7],\n",
       "       [  1. ,  59. ,   2.6],\n",
       "       [  1. ,  60. ,   2.8],\n",
       "       [  1. ,  61. ,   3.1],\n",
       "       [  1. ,  62. ,   2.4],\n",
       "       [  1. ,  63. ,   3. ],\n",
       "       [  1. ,  64. ,   2.5],\n",
       "       [  1. ,  65. ,   2.7],\n",
       "       [  1. ,  66. ,   2.6],\n",
       "       [  1. ,  67. ,   3. ],\n",
       "       [  1. ,  68. ,   2.7],\n",
       "       [  1. ,  69. ,   2.8],\n",
       "       [  1. ,  70. ,   2.5],\n",
       "       [  1. ,  71. ,   2.5],\n",
       "       [  1. ,  72. ,   2.4],\n",
       "       [  1. ,  73. ,   2.5],\n",
       "       [  1. ,  74. ,   2.8],\n",
       "       [  1. ,  75. ,   3. ],\n",
       "       [  1. ,  76. ,   3.2],\n",
       "       [  1. ,  77. ,   2.6],\n",
       "       [  1. ,  78. ,   3.4],\n",
       "       [  1. ,  79. ,   2.4],\n",
       "       [  1. ,  80. ,   2.7],\n",
       "       [  1. ,  81. ,   2.5],\n",
       "       [  1. ,  82. ,   3. ],\n",
       "       [  1. ,  83. ,   3. ],\n",
       "       [  1. ,  84. ,   2.9],\n",
       "       [  1. ,  85. ,   3.1],\n",
       "       [  1. ,  86. ,   2.6],\n",
       "       [  1. ,  87. ,   3.1],\n",
       "       [  1. ,  88. ,   2.5],\n",
       "       [  1. ,  89. ,   2.6],\n",
       "       [  1. ,  90. ,   2.9],\n",
       "       [  1. ,  91. ,   2.5],\n",
       "       [  1. ,  92. ,   2.8],\n",
       "       [  1. ,  93. ,   2.9],\n",
       "       [  1. ,  94. ,   3.1],\n",
       "       [  1. ,  95. ,   3.1],\n",
       "       [  1. ,  96. ,   2.5],\n",
       "       [  1. ,  97. ,   3.3],\n",
       "       [  1. ,  98. ,   2.7],\n",
       "       [  1. ,  99. ,   2.8],\n",
       "       [  1. , 100. ,   2.9],\n",
       "       [  1. , 101. ,   3. ],\n",
       "       [  1. , 102. ,   2.5],\n",
       "       [  1. , 103. ,   2.8],\n",
       "       [  1. , 104. ,   3.3],\n",
       "       [  1. , 105. ,   3.2],\n",
       "       [  1. , 106. ,   3.3],\n",
       "       [  1. , 107. ,   3.3],\n",
       "       [  1. , 108. ,   2.5],\n",
       "       [  1. , 109. ,   2.9],\n",
       "       [  1. , 110. ,   2.9],\n",
       "       [  1. , 111. ,   3.3],\n",
       "       [  1. , 112. ,   3.1],\n",
       "       [  1. , 113. ,   2.7],\n",
       "       [  1. , 114. ,   2.8],\n",
       "       [  1. , 115. ,   2.7],\n",
       "       [  1. , 116. ,   2.5],\n",
       "       [  1. , 117. ,   3.2],\n",
       "       [  1. , 118. ,   2.9],\n",
       "       [  1. , 119. ,   2.5],\n",
       "       [  1. , 120. ,   3.1],\n",
       "       [  1. , 121. ,   2.8],\n",
       "       [  1. , 122. ,   3.3],\n",
       "       [  1. , 123. ,   2.9],\n",
       "       [  1. , 124. ,   3.3],\n",
       "       [  1. , 125. ,   3. ],\n",
       "       [  1. , 126. ,   2.5],\n",
       "       [  1. , 127. ,   2.6],\n",
       "       [  1. , 128. ,   3.1],\n",
       "       [  1. , 129. ,   3.3],\n",
       "       [  1. , 130. ,   2.7],\n",
       "       [  1. , 131. ,   3.3],\n",
       "       [  1. , 132. ,   3. ],\n",
       "       [  1. , 133. ,   3. ],\n",
       "       [  1. , 134. ,   2.8],\n",
       "       [  1. , 135. ,   3.2],\n",
       "       [  1. , 136. ,   3.1],\n",
       "       [  1. , 137. ,   3.3],\n",
       "       [  1. , 138. ,   2.8],\n",
       "       [  1. , 139. ,   2.9],\n",
       "       [  1. , 140. ,   2.7],\n",
       "       [  1. , 141. ,   2.5],\n",
       "       [  1. , 142. ,   2.7],\n",
       "       [  1. , 143. ,   2.5],\n",
       "       [  1. , 144. ,   3.4],\n",
       "       [  1. , 145. ,   2.8]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(x):\n",
    "    return 1/(np.exp(-x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_pred):\n",
    "    return (y == y_pred).astype(int).sum()/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_, y_, batch_size, epochs, eta):\n",
    "    theta = np.random.rand(X.shape[1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print('accuracy: ', accuracy_score(y_, sigm(theta.T@ X_.T).squeeze()>0.5))\n",
    "        rand_idxs = np.random.randint(X.shape[0], size=batch_size)\n",
    "        X_Batch = X_[rand_idxs]\n",
    "        Y_batch = y_[rand_idxs]\n",
    "        grad = 0\n",
    "        batch_loss = 0\n",
    "        for xx, yy in zip(X_Batch, Y_batch):\n",
    "            grad += xx*(sigm(theta.T@xx)-yy)\n",
    "            batch_loss += (1-yy)*np.log(1-sigm(theta.T@xx) + 1e-6) + yy*np.log(sigm(theta.T@xx)+ 1e-6)\n",
    "        print('batch loss: ', -batch_loss)\n",
    "        theta = theta - (1/X_Batch.shape[0])*eta*grad\n",
    "    \n",
    "    return theta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5051546391752577\n",
      "batch loss:  428.3291242182837\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  414.4965587831988\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  497.44013368222926\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  497.4283899328323\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  414.5025760796793\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  373.24929704112304\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  372.83298315153763\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  452.02295368613466\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  351.7497862308041\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  319.26536124382113\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  335.8494911004045\n",
      "accuracy:  0.5051546391752577\n",
      "batch loss:  152.63028252941936\n",
      "accuracy:  0.5773195876288659\n",
      "batch loss:  46.38437286719768\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.245217617624405\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.668088908109404\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.058453067027344\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.95059324495861\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.786109662304575\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.44539345973594\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  38.23958306532048\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.372772735469304\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.76564130839248\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  26.18279004070761\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.44833195974558\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  32.18413141908341\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.94834755451341\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.468789228210223\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.51661534521832\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.125645995232276\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.424377706054074\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.23546469465109\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.58106714560776\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.76685499009636\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.1577241486204\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.07552198937531\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  35.13925805658093\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.605376943201886\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.178525341314785\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.596500107880267\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  39.77190999259593\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  31.696662701801415\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  31.95547064254727\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.607728561620572\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.18157265920843\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.244703576665565\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.67968179668611\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.777716273031864\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.02423342848876\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.599099880000512\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.510303885962756\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.292262687122562\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.29740898491235\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  32.58339493471629\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.46588964707812\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.901891089131812\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  39.4695606650023\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  31.533612076780766\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.33516183467115\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.33214011385104\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.86335232835157\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.368708925702624\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.331342728592716\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.17581794946198\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.86757880798089\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.73807527707664\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.55285988748004\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.927929922277386\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.0744507319713\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.74352084375624\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.54249797062828\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.97394290782672\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.22123213626413\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  38.23766994219821\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  31.981391154308817\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.93851446552607\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  41.1257222705449\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.58250240218663\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.972413779552145\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.628940150276183\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  42.3978796256897\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.569648428766136\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.694380847558367\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.547971671263056\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.107905376699993\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.22269522167109\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.96024576869803\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.7768705373239\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  30.64795052990911\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  22.84070369906768\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.62336080252574\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  41.69164945994341\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  35.31116525017462\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.84199252169038\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  37.42580088775814\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  25.529367073125034\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.27539903068297\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.485355485342538\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.057606837296557\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.38082406250097\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  33.16967288561363\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.295757586952604\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.256724739802074\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.464384643733204\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  34.45785153909289\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  26.91071610541384\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.62569885141227\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  28.705174229324246\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.28775135057915\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  40.17687748006267\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.05708091916948\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.400050096251547\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.31309312976805\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.481503770561957\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.68036066093993\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.274417728482646\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.680796796700285\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  37.48740714517718\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  41.45573902948421\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  30.83876149307884\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.457551152417516\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.188017256165868\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.29124005910464\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.571386312007085\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.60094891871353\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.350290615761075\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.08704341588642\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.77682366209085\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.371081748550885\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  43.46409064424227\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  32.63386632796357\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.394416478544166\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  41.96703865366587\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.291101128110476\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.98863143368308\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.42809442571285\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  35.624963057409545\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  44.03637460394467\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  34.55077470256052\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.165826949848412\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.78410525493674\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.08231434649881\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  31.184318103768234\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.01890369878843\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  33.15936115574976\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.127094081941056\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  41.162578205398596\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  29.358739707814273\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.84768416077056\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.714326690026237\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.381533090297886\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.927477533945698\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.78542462691304\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.824105042960404\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.612423991520835\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.705817278102245\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.67266030755863\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.88654105023376\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.07532689072357\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  40.926132015736314\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.14364993378817\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.372692438175562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.8031206335207\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.230471164829495\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.58892722101826\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.96146318794576\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.933160688413565\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.174603358367726\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.8086300167399\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  22.95956314189852\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.559007729700213\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.61664127970845\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.15962338471855\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  24.14084611019515\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  23.99518221923981\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.600587331479026\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.348587775050134\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.1845405319497\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.892579073749676\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.8375412615257\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.311174491810068\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  19.957008547346394\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  38.23852344432637\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.355908732437808\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.023748957005985\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.224741237101103\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.917097416858525\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.346699915511394\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.27896554266909\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.135079113613365\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.59251835493514\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  40.10655570994863\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  36.82469030876567\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.66932489357071\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  27.68593527387221\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.156697817683906\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.67826560719351\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.74895689383261\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.376780472613813\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.423826472914428\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.908717482592177\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.83436127361707\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.48596042640609\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.36937838780662\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.42623824368827\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.69938571308233\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.40464778857419\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  28.919768017737805\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.66718850778016\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.140699730401195\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  39.1994194457062\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  30.04764191459791\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  37.91523402299785\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.97012853473362\n",
      "accuracy:  0.6804123711340206\n",
      "batch loss:  38.73494105675155\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.903086889873308\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.9039694479749\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  39.31710244631207\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  34.67276892472387\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.559914206707916\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.41649902599965\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.649953362304295\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  34.763414857456254\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.34289358601796\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.83638306258508\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.447637407024946\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.056552697073958\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.388258244667455\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.80035467859936\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.23858992787704\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.030582220881183\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.03441023655065\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.22660212717956\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.37149344561216\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.498007752327823\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.931889920819465\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.1647701328285\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.51204845802293\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.016928465148013\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.86302770103516\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.616591218073975\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.21818898161226\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.205559307093953\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.782447683039084\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.58062902470103\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  37.364662759887004\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.982990417940364\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.720586996561636\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.607491296037107\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.770910773314945\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.58060522536946\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  34.968468066123265\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.94028299456811\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  37.27025900546329\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.950001638905064\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.94364712344453\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.15380985756644\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.270685831027933\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.98770608885647\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  34.59547577811876\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.10903329800962\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.28251397552096\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.31846886114136\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.196542489428154\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.111944037028007\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.38087978768594\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  29.140191845984965\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.61729351920426\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.258361061258146\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.9100751616141\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  36.26481936045553\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.74594972958746\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.05661944716989\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  35.791545942418125\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.40083430998694\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  32.37056030873381\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.862603447617914\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.882487072577646\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.986316330838726\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.56673782032434\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.614377176423265\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.6687995804076\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.06583618758175\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  29.969612085883085\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.575921496164966\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.551895857539684\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.00843154862436\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.70192946832098\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.34145636835587\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  39.09661690545618\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  29.772300838536744\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.483667660293335\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.47758350999839\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.779851598244605\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.4128004075289\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  30.85528111120691\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.58567868395612\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.720295162636305\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  24.540313725754604\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.850245913950452\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.609273283637926\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  40.45219704778883\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  26.290976120355868\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  39.01528699606673\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.037714581600575\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  26.605762424628736\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.30109405889692\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  32.427077245854036\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.80180281498576\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.62671635931672\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  41.31320925353826\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.258481045965205\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.86984536687338\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  36.17675409666397\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  39.66072398379503\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  32.325366845340795\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.525885473439153\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.10483683370758\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.874649161746678\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.77290985214462\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.41992948212179\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.73513171139634\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.529499044589286\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.49375360857958\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  29.648728527838404\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  27.49518968746678\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.06443815433178\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.400418139905028\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.51466738110009\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  25.950502157834674\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.053474396569825\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.18006471493419\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.929648271640175\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  25.197646367420504\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.21687283072895\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.20249187036733\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.21339328231338\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.69074404049405\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.41508945177614\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.20262196337611\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.369362248009434\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.020604422795063\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.26312268063548\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.25085761268385\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  30.593648731208873\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.846738237649976\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.156197240190664\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.340146386369575\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.430971972198144\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.28257084549014\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.261841226826807\n",
      "accuracy:  0.7371134020618557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss:  38.56248641551242\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.21503742266086\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  32.81984290415092\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.014041817232137\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.73544848652485\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  32.16462072500961\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.45393392360484\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.2582898900169\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  24.825782351479088\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.61133285872662\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  35.80948781965547\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.56180936692009\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.741672948484215\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.88334944319512\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.897231840029512\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.37588614411052\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.30419714555229\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.43004555647597\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.428693361987854\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.969622040306575\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.111893628804374\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.65245418345557\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.79715213103527\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.084789596474465\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.085969202447515\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  42.552958633469615\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.809145270257822\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.26277873486618\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.84619147528502\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.098919581251423\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.062564787068467\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.521220288592087\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.46983089144768\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.08240739735237\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.65875826609904\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  32.31421992444711\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.980798390636277\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.28783847076523\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.72890721975763\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  32.056391111152934\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.30384024906881\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.59887280631273\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  30.580212789419157\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.826481315109824\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  34.10509774976306\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.357761739936436\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.26123043743317\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.13478273882669\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.467171582171723\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.287124787092864\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  40.708429192758864\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.400657076960876\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.27400123471138\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.189967409453704\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.23781813369426\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  42.275599453971594\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  34.16079148459161\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.529415245312958\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.58534360389106\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.79833932157395\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.40418892176442\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.10162520454356\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  38.439415735894585\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.006263333865704\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.86921333264435\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  34.42134620208242\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  40.471041005097184\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.21918449546798\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.47276903818653\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.181553822905414\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.11721872846069\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.15569617323651\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  38.27537627853218\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.747336465184354\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.901030506837508\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.0940082209766\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.161340946095994\n",
      "accuracy:  0.6649484536082474\n",
      "batch loss:  37.90044489689705\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.329518212608686\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.185625084921945\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.1866802731813\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.390016807207417\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.84914573496434\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.88417757305421\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.10247513976786\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.63802238652188\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.932057307421715\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.521261282246485\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.6542557749335\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.05278978431622\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.064689969556056\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.76693459987519\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.77829438553328\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.4609180625768\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.614792255924677\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.08340804414209\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.39857306519938\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.04979055237501\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.77352046203311\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.66453654657345\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  38.375019152699146\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.995573608480335\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.51030184291957\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.23142235656824\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.15813512744385\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  32.91888668325798\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.987928442265964\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.018021527868065\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.91424868859532\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.22431525473325\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.945818131380435\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.43630248077663\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.051370085471795\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  41.651069962951574\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  34.369949592517074\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.62469487251667\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.92238172646173\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.74302716613516\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.271235683719198\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.226669055061787\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.24190319554042\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.73293479182162\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  38.87523747303401\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  35.02459667281979\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.250855848880587\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.502631656655126\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.408155146179784\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.122172251459897\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.25020607768123\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.142511440659504\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.776010716005274\n",
      "accuracy:  0.6649484536082474\n",
      "batch loss:  39.19460122482918\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  38.170363808724744\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.428319475357874\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.442171463365163\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.528593595034504\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.5696427906051\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  35.7003564390229\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.474606147998266\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  38.4257010956464\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  31.624410067445446\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  29.500047580859544\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  32.539303505686036\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.273579318696758\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.44067050548807\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.346027876535175\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  37.07354479804712\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.57257124477416\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.646980876705467\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.514529594024285\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.20424359988897\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  40.471895445501914\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.02484067520931\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.14149335916127\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.260587449323452\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.22516100460468\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.50232637559419\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.185470309047474\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.568970640371962\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.994653517602526\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.014184665177\n",
      "accuracy:  0.7164948453608248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss:  22.486779904625042\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  42.97350290523539\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.038012676463772\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.271373334132434\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.161180827014576\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.2455121660541\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  38.602652397626684\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.760305016931426\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.75023731849289\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  37.88239507406949\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.89873164372308\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.81901035785143\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  36.425893308835455\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  22.087757620759675\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  26.631918341423216\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  42.56486314926413\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.31647610435558\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.62831371661326\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  27.27258756844349\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.14635200211166\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.96937852160784\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.572349697168075\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.52486093839907\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  28.068547338170443\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.24803731977702\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.58187057930746\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.626844042158524\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.06108859252133\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.354607030317638\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  27.242239974049024\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  40.4321850588241\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.303710128307365\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  33.33689272673331\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.6425404913636\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.165118112392015\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.48757868590319\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  37.84126051023666\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  38.587422485028036\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.75762937235894\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.78432504517494\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  35.74134734415091\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.699477437716205\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.8345329799395\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  37.146301995508836\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.50317255238932\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.49903368300508\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.969449265867414\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.34000985625336\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  34.80873928540631\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.388406627745095\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.046247049159906\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  40.44008210828259\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  26.0729752984645\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.41616446966348\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.02174841739396\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  29.864215764637578\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.816128938668538\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.685856121315474\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  39.02260761387915\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.211041183627216\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.399042918403225\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  26.59605302267474\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.203074209805106\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.1592688743973\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  42.2043576140208\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.922768010859624\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  40.398791566277296\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.563320908950555\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.1234697771122\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  41.361576977618135\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.725251641275445\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.899603434981\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.039283784763654\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.254204297208055\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.772316298089216\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  26.182948469801797\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.54353176823325\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  36.51149512424276\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.729113311893705\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.50802291135685\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  38.59420002051517\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.83713238321812\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.439587482210598\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  36.46947069441935\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.195392353720212\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.05790839262851\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.0744794930387\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  28.03889833341892\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.52890188142826\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.876523472399846\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.054036386783782\n",
      "accuracy:  0.6804123711340206\n",
      "batch loss:  35.30119303028149\n",
      "accuracy:  0.6855670103092784\n",
      "batch loss:  35.80765514429475\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.353759967630562\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.45847164932852\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.508112207895245\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  37.50693459915929\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  26.249685489185104\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.911987440359336\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  35.16192667558955\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  42.62430401574289\n",
      "accuracy:  0.6701030927835051\n",
      "batch loss:  34.92356123258394\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.407287976276514\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.82298257882786\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.18829703576161\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.37027490360033\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.102749073978913\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  35.208186237304204\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.210516313974594\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.64408989116099\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.722608880439022\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.484778808303496\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.94204277857233\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.06830689855007\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.39414764677156\n",
      "accuracy:  0.6804123711340206\n",
      "batch loss:  38.00380144013284\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.126528283713572\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.39046161356506\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.579755206152075\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.946910390282124\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  23.81271092118176\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.881820700205637\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.358678261859865\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.7507607508165\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.13289414919541\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  27.476811631868866\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.65430145788655\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  35.37365013214689\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.78024991405435\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.51609656750871\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.613015701429713\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.95708101409706\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.61931032434219\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.46147926408384\n",
      "accuracy:  0.6855670103092784\n",
      "batch loss:  37.31141849917775\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.20197705358638\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.729016364231924\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.234836939818294\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.43702649849231\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.817329124522843\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.658536174617936\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.616970391063408\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.35532826424791\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  32.4019624906738\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.350490183858582\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  39.26989931124463\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  40.6736891384426\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.242516739399846\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.474264914429156\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  24.32247946275414\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.694812731252\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.48113442110803\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  25.767159284808447\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.606086455374374\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  25.735672602565188\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  40.71023437318606\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.62349241204019\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.13175993814455\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.25760901106485\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.696733124672022\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.8293199841998\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.48855754409166\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.539594689897427\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  40.48505879788079\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.068952328238495\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.133261870741144\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.646335111168796\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.749679749157806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7422680412371134\n",
      "batch loss:  24.913720327578176\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.63980615131902\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  41.42398437939885\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  30.40635221238655\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.22858263566716\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.833132414177733\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.586876795512023\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.87382473713933\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  23.776785233802194\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.114836411753828\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  39.20308630392983\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.06225244655669\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.958430347063377\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.160674158043587\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.309066758551204\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.533730874273786\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.903984761724693\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.21933864845785\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.047189888175524\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.452225012232574\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.03753156023891\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.69051178235366\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.65331345794023\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.59677453883637\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.76946932984391\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.00588263792835\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  34.094187394569964\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.107193833653124\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.43902650989451\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.09130882158136\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.22099634659675\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.70218416387389\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.632185964500714\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  32.66562808607469\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.42858924699953\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.58420590981048\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.802354906844826\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.28918013368114\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.300578843346262\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.5503739439471\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.89218815106198\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.6901788147615\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.89443201388145\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.33982492195935\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.884606479991916\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  25.990120518936436\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.43733415120366\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  39.969357928383374\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.6454003206243\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.80971831320864\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.88784924280524\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.160404044346144\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  36.70074681916448\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.12161071518904\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.90057976447564\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.42984206235699\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.63284825354488\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.162659541917165\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.25597993401375\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.36990922671383\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.667152337227886\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  28.636016613982616\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  31.03420862606744\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.18822016137371\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  33.04296862588577\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.972337944162458\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.60876751453238\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  28.781360076916297\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.480160763342372\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.929310099646205\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.704509424558445\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.001077169196485\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.12258134279039\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.185442934578774\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  23.718323965166682\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.403050233651694\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.150746060753125\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.50610242028695\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.22401940925432\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.763402981373222\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.551607379314113\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  39.6026311327491\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.970272143376622\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.629322071325745\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.128101479535474\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.013150986603065\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.00892260678173\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.498656674761385\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.63681223626505\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  31.412107172563992\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  41.658176094062796\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  30.559856520479798\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  37.57539245572676\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.25029338588914\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.342177519241964\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  39.17099578585307\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.24984541190968\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.947190590498877\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.178579634214074\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.3070997872121\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  34.95143106368068\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.96359046741842\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.822864302192745\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.03540501340332\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  27.791057106630433\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.931005445313215\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.7375081037709\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.07214449641922\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.766076209327974\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.16458469598208\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  36.294491338683876\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  25.27606332061576\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.282539012248332\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.094017929676287\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.22000177846253\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.96966032220574\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.645224139326437\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.06238874858194\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  30.267872016556463\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.59771563727854\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  33.35199490317981\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.2205119435141\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.82105657782239\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.99028715895098\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.873886987620832\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.226203880103697\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.77836134356791\n",
      "accuracy:  0.6855670103092784\n",
      "batch loss:  31.51517382137661\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.813496962866395\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  34.504891407826605\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  22.765243936208776\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.08711160724188\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.488795257854374\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  34.76835007498326\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.224460243479164\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.35101166818097\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.45049785764792\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  31.83077388318393\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.404537340717383\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  33.963697638417635\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.60343619363359\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.16880461972988\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.629254250865774\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.51797852590794\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.72032501149655\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  34.55549394778308\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.96871753539922\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.703913151022796\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  26.994090780004235\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.069986907589143\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.64903408274437\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  38.21843293855366\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  37.46278589624454\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.618389428882292\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  33.785711653551374\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  36.86376506450744\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  32.724880100271406\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  31.54252965951464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7422680412371134\n",
      "batch loss:  39.69984157738361\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.82268131342022\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.01032703899673\n",
      "accuracy:  0.6907216494845361\n",
      "batch loss:  32.32520461608677\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.64186964614233\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.507743561815246\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.588864754894207\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  39.494754583695446\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.47684305082917\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.70218431380312\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.804852396969245\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.06566853348643\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.649952318206324\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.66432649596506\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.601879478777214\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.02594908489862\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.346767212821746\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.36429976117071\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.039516481901806\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.644048102907554\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  30.792505738766913\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  37.96714176955954\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.19610020438854\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  31.16126534165786\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.263927001899226\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.510262153397093\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  34.65564182443109\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.93336570308583\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  41.91539973153705\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.68732056478096\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  26.821301535539817\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.771580786479745\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.9714534504584\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  36.572707783416675\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.625971851441932\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.294780696674184\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.091648471653336\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  35.36489785765192\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.71394766911765\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  32.345114585674565\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  24.734091803850053\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.918271143913415\n",
      "accuracy:  0.7061855670103093\n",
      "batch loss:  36.42740474361856\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  36.81086394689889\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  37.26065156021336\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  26.976634573060455\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  24.95838327383713\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  42.56982830348393\n",
      "accuracy:  0.6649484536082474\n",
      "batch loss:  38.60141355255686\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  29.670598677293103\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  30.818684000785648\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.10963756442645\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.073753395550455\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.32099886349994\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.95381371198945\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  29.327253552205804\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.159284074827454\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  25.888330961148196\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  24.69982701987293\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.841016216666972\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.64767608772457\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  27.85296400272828\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  27.691593376942667\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.52434674182188\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  30.769259768733075\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.105858033669804\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  31.523294241790396\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.671059378791995\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.511479799052005\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.756865596858162\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  27.89950216514738\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.36267478174921\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  32.56059624391504\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.494148441314408\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.770677334422682\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  41.36065686785683\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.4466768472298\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.861286629687918\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.05501832531486\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.647032934971314\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.49171699967771\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.672871426052346\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.85709034017805\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.908961504276053\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.010220673499234\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  32.64449325115198\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  35.72193953928109\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  26.36090037418326\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.13983317216944\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.603616761507553\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  30.180713080715744\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  28.65586288965081\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.96527147853576\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.0834640057819\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.87018336423522\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.56543312351595\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.82194960864977\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.300568040476236\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.95304579964603\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  28.55512007236555\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.032157035079145\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.38768785048428\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.209993269363284\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  32.734689921393226\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.02271051010744\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.55418330371891\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.414439960426307\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  34.147979161699396\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  37.53381497437899\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.09935179159049\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  30.306959974812624\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  34.33330940665845\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  26.291302518767576\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.544654306768845\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.373113598242256\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  29.235581925682467\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.152815442429954\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.806773515291727\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  42.68069022724898\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  29.08240672147515\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.73317731210625\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  32.84789059199351\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.73847086918169\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  25.80777363713883\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.35977633863543\n",
      "accuracy:  0.7164948453608248\n",
      "batch loss:  35.61721347698525\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  31.62694691921422\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  38.007348579107585\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.18721244098085\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.85905393934798\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.20959803675027\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  36.79404266925403\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.025952728255554\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  28.829469531304962\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  38.43134454440713\n",
      "accuracy:  0.7010309278350515\n",
      "batch loss:  34.331702736750096\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  29.11539634711616\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  36.84373854939531\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  29.566801060285663\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.52206712077336\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.48944639622159\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  30.298889093865736\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.120740684774766\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  35.357568819774414\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  30.05478625813246\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  31.663511844097616\n",
      "accuracy:  0.7422680412371134\n",
      "batch loss:  35.23610447132153\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  25.158001826329844\n",
      "accuracy:  0.7371134020618557\n",
      "batch loss:  32.17358062353029\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  39.32932537702815\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.83038252117945\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  33.2254327636601\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  28.45195614822254\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  32.32373244371673\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  39.67408767996828\n",
      "accuracy:  0.6701030927835051\n",
      "batch loss:  34.95344186321313\n",
      "accuracy:  0.7268041237113402\n",
      "batch loss:  37.58188458302223\n",
      "accuracy:  0.711340206185567\n",
      "batch loss:  30.16788797420832\n",
      "accuracy:  0.7216494845360825\n",
      "batch loss:  33.48626884745135\n",
      "accuracy:  0.6958762886597938\n",
      "batch loss:  33.88288336315664\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  35.68595082180156\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  33.55056065059169\n",
      "accuracy:  0.7319587628865979\n",
      "batch loss:  40.497291022567644\n"
     ]
    }
   ],
   "source": [
    "theta = train(X, y, 64, 1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "\n",
    "y1_test = np.array([np.random.rand(1) + 2 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "y2_test = np.array([np.random.rand(1) + 2.4 for c in (range(np.random.choice([c+60 for c in range(40)])))])\n",
    "\n",
    "x1_test = np.array(range(len(y1_test)))\n",
    "x2_test = np.array([c+50 for c in range(len(y2_test))])\n",
    "\n",
    "a1_test = np.concatenate((x1_test.reshape(-1,1),y1_test,np.array([1 for c in range(len(x1_test))]).reshape(-1,1)), 1)\n",
    "a2_test = np.concatenate((x2_test.reshape(-1,1),y2_test,np.array([0 for c in range(len(x2_test))]).reshape(-1,1)), 1)\n",
    "\n",
    "A_test = np.concatenate((a1_test,a2_test), 0).round(1)\n",
    "np.random.shuffle(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = np.ones((A_test.shape[0], A_test.shape[1]+1))\n",
    "temp_test[:,1:] = A_test\n",
    "X_test = temp_test[:,:-1]\n",
    "y_test = temp_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_logistic_regr(theta, data):\n",
    "    return (sigm(theta.T@ data.T).squeeze()>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pred_logistic_regr(theta, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7635135135135135"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
